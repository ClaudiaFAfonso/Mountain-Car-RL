{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Mountain Car Problem</center></h1>\n",
    "\n",
    "<h2><center>Part 2: Deep Learning</center></h2>\n",
    "\n",
    "<h3><center>Claudia Afonso</center><h3>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This second notebook contains the part of the project related with Deep Q-learning. The first notebook contains the part of the project related with Q-learning. The comparison between the two trained agents can be found in this notebook at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training an agent with Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind deep Q-learning (DQN) consists of combining Q-learning with deep learning. Instead of using the Q-table to store and update the values of the state-action pairs, neural networks are used to approximate the optimal action value function, $Q_{opt}(s,a)$. The network receives a given state as input and produces the estimated Q-values as outputs (one for each action).\n",
    "\n",
    "To train the network, the ground truth is created on the run by using the Bellman equation:\n",
    "\n",
    "<br>\n",
    "<center>$Q(s,a) = r + \\gamma \\max_{a} Q(s',a)$</center>\n",
    "\n",
    "Thus, here we are approximating the current action value $Q(s,a)$ based on an estimate of the future $Q(s',a)$ value. Therefore, two separate networks need to be used, the first to estimate the current values and the second to estimate the future (target) values. The weights of the first network (the policy network) are updated to move the predictions closer to the target Q-values. Every C time step, the weights from the policy network are copied to the second network (the target network). This two-network solution provides stability to the DQN algorithm, since it uses separate networks to produce estimates (one for the current Q-value and another for the future Q-value).\n",
    "\n",
    "Successive experiences in a sequence of interactions are highly correlated with each other and this can lead to instability during training. Thus, the DQN algorithm uses the biologically-inspired `Experience Replay` mechanism, which serves to store past experiences (that is, state, action, reward, future state values). This experience replay is sampled according to a given batch size to retrieve past experiences, thereby randomizing and breaking the temporal correlation in the data.\n",
    "\n",
    "The pseudocode for implementing the DQN algorithm is the following:\n",
    "\n",
    "<img src=\"Pseudocode_DQN.jpg\" width=\"500\" height=\"340\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Applying Deep Q-Learning to the Mountain Car problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the tutorial at TensorFlow (https://www.tensorflow.org/agents/tutorials/1_dqn_tutorial) was followed to implement a DQN agent to the Mountain Car problem. Since DQN can handle problems with continuous state spaces, it was not necessary to perform discretization as done in the case of Q-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ns8CKfCLOrxK",
    "outputId": "a918691e-71a3-45ce-dc51-43d803fd421d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "0% [Working]\r",
      "            \r",
      "Hit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
      "\r",
      "0% [Waiting for headers] [Connected to cloud.r-project.org (52.85.151.54)] [Con\r",
      "                                                                               \r",
      "Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
      "\r",
      "                                                                               \r",
      "Hit:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
      "\r",
      "0% [Waiting for headers] [Connected to cloud.r-project.org (52.85.151.54)] [Con\r",
      "                                                                               \r",
      "Hit:4 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
      "\r",
      "0% [Connected to cloud.r-project.org (52.85.151.54)] [Connecting to ppa.launchp\r",
      "                                                                               \r",
      "Hit:5 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
      "Hit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
      "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
      "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
      "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
      "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "freeglut3-dev is already the newest version (2.8.1-6).\n",
      "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
      "xvfb is already the newest version (2:21.1.4-2ubuntu1.7~22.04.5).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 24 not upgraded.\n",
      "Requirement already satisfied: imageio==2.4.0 in /usr/local/lib/python3.10/dist-packages (2.4.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from imageio==2.4.0) (1.23.5)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from imageio==2.4.0) (9.4.0)\n",
      "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.10/dist-packages (3.0)\n",
      "Requirement already satisfied: tf-agents[reverb] in /usr/local/lib/python3.10/dist-packages (0.19.0)\n",
      "Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (1.4.0)\n",
      "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (2.2.1)\n",
      "Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (0.5.0)\n",
      "Requirement already satisfied: gym<=0.23.0,>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (0.23.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (1.23.5)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (9.4.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (1.16.0)\n",
      "Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (3.20.3)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (1.14.1)\n",
      "Requirement already satisfied: typing-extensions==4.5.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (4.5.0)\n",
      "Requirement already satisfied: pygame==2.1.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (2.1.3)\n",
      "Requirement already satisfied: tensorflow-probability~=0.23.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (0.23.0)\n",
      "Requirement already satisfied: rlds in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (0.1.8)\n",
      "Requirement already satisfied: dm-reverb~=0.14.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (0.14.0)\n",
      "Requirement already satisfied: tensorflow~=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (2.15.0)\n",
      "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from dm-reverb~=0.14.0->tf-agents[reverb]) (0.1.8)\n",
      "Requirement already satisfied: portpicker in /usr/local/lib/python3.10/dist-packages (from dm-reverb~=0.14.0->tf-agents[reverb]) (1.5.2)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents[reverb]) (0.0.8)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (3.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (3.3.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (23.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (67.7.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (2.4.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (0.35.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (1.60.0)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (2.15.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (2.15.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability~=0.23.0->tf-agents[reverb]) (4.4.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow~=2.15.0->tf-agents[reverb]) (0.42.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (2.17.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (3.5.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (3.0.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from portpicker->dm-reverb~=0.14.0->tf-agents[reverb]) (5.9.5)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (2023.11.17)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (3.2.2)\n",
      "Requirement already satisfied: pyglet in /usr/local/lib/python3.10/dist-packages (2.0.10)\n",
      "Requirement already satisfied: tf-keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\n"
     ]
    }
   ],
   "source": [
    "!sudo apt-get update\n",
    "!sudo apt-get install -y xvfb ffmpeg freeglut3-dev\n",
    "!pip install 'imageio==2.4.0'\n",
    "!pip install pyvirtualdisplay\n",
    "!pip install tf-agents[reverb]\n",
    "!pip install pyglet\n",
    "!pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "bDCmJCEVPLOE"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# Keep using keras-2 (tf-keras) rather than keras-3 (keras).\n",
    "os.environ['TF_USE_LEGACY_KERAS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "0so6zC76OZdb"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import base64\n",
    "import imageio\n",
    "import IPython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import pyvirtualdisplay\n",
    "import reverb\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.drivers import py_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import sequential\n",
    "from tf_agents.policies import py_tf_eager_policy\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import reverb_replay_buffer\n",
    "from tf_agents.replay_buffers import reverb_utils\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.utils import common\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "from google.colab import files\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "K9FlCn_DPI-B"
   },
   "outputs": [],
   "source": [
    "# Set up a virtual display for rendering OpenAI gym environments.\n",
    "display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 Defining the Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "YWC3qqmxPSXo"
   },
   "outputs": [],
   "source": [
    "num_iterations = 500000 # @param {type:\"integer\"}\n",
    "\n",
    "initial_collect_steps = 100  # @param {type:\"integer\"}\n",
    "collect_steps_per_iteration = 1 # @param {type:\"integer\"}\n",
    "replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n",
    "\n",
    "batch_size = 64  # @param {type:\"integer\"}\n",
    "learning_rate = 1e-4  # @param {type:\"number\"}\n",
    "log_interval = 200  # @param {type:\"integer\"}\n",
    "\n",
    "num_eval_episodes = 10  # @param {type:\"integer\"}\n",
    "eval_interval = 1000  # @param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A linear epsilon decay as the number of iterations during training increased was also used, as previously done for Q-learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "J7wHXnzR8_Ms"
   },
   "outputs": [],
   "source": [
    "global_step = tf.Variable(0, trainable=False)\n",
    "epsilon_init = 0.55\n",
    "min_epsilon = 0.05\n",
    "decay_steps = num_iterations\n",
    "epsilon = tf.compat.v1.train.polynomial_decay(epsilon_init,\n",
    "                                              global_step,\n",
    "                                              decay_steps,\n",
    "                                              min_epsilon,\n",
    "                                              power=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "DZP_H9Rg984Y"
   },
   "outputs": [],
   "source": [
    "gamma = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3 Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "kxrJplaXPTWr"
   },
   "outputs": [],
   "source": [
    "env_name = 'MountainCar-v0'\n",
    "env = suite_gym.load(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "id": "8E057s_4PVgb",
    "outputId": "15310125-ca3f-49b5-c9d3-55a05ce563d1"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=600x400>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()\n",
    "PIL.Image.fromarray(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "nE1bAUwGPYdA"
   },
   "outputs": [],
   "source": [
    "train_py_env = suite_gym.load(env_name)\n",
    "eval_py_env = suite_gym.load(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "wCQKycmpPcpo"
   },
   "outputs": [],
   "source": [
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.4 Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "UhUfW6FlPecc"
   },
   "outputs": [],
   "source": [
    "fc_layer_params = (100, 50)\n",
    "action_tensor_spec = tensor_spec.from_spec(env.action_spec())\n",
    "num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n",
    "\n",
    "# Define a helper function to create Dense layers configured with the right\n",
    "# activation and kernel initializer.\n",
    "def dense_layer(num_units):\n",
    "    return tf.keras.layers.Dense(\n",
    "        num_units,\n",
    "        activation=tf.keras.activations.relu,\n",
    "        kernel_initializer=tf.keras.initializers.VarianceScaling(\n",
    "            scale=2.0, mode='fan_in', distribution='truncated_normal'))\n",
    "\n",
    "# QNetwork consists of a sequence of Dense layers followed by a dense layer\n",
    "# with `num_actions` units to generate one q_value per available action as\n",
    "# its output.\n",
    "dense_layers = [dense_layer(num_units) for num_units in fc_layer_params]\n",
    "q_values_layer = tf.keras.layers.Dense(\n",
    "    num_actions,\n",
    "    activation=None,\n",
    "    kernel_initializer=tf.keras.initializers.RandomUniform(\n",
    "        minval=-0.03, maxval=0.03),\n",
    "    bias_initializer=tf.keras.initializers.Constant(-0.2))\n",
    "q_net = sequential.Sequential(dense_layers + [q_values_layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "r1asnfqAPijq"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    gamma=gamma,\n",
    "    epsilon_greedy=epsilon,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter,\n",
    "    target_update_tau=0.005,\n",
    "    target_update_period=5)\n",
    "\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.5 Policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A policy defines the way an agent acts in an environment. Typically, the goal of reinforcement learning is to train the underlying model until the policy produces the desired outcome. In the case of the Mountain Car problem, the desired outcome is for the car to reach the flag on the right hill. \n",
    "\n",
    "For each time_step observation, the policy returns an action:\n",
    "* `0: Accelerate to the left`\n",
    "* `1: Don’t accelerate`\n",
    "* `2: Accelerate to the right`\n",
    "\n",
    "Agents contain two policies: \n",
    "\n",
    "* `agent.policy` - the main policy that is used for evaluation and deployment\n",
    "\n",
    "* `agent.collect_policy` - a second policy that is used for data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Jqd0s1n4PlRH"
   },
   "outputs": [],
   "source": [
    "eval_policy = agent.policy\n",
    "collect_policy = agent.collect_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policies can be created independently of agents. In this case, a `random_policy` was created that randomly selects an action for each time_step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "xctKm_J9PndR"
   },
   "outputs": [],
   "source": [
    "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
    "                                                train_env.action_spec())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an action from a policy, call the policy.action(time_step) method. The time_step contains the observation from the environment. This method returns a PolicyStep, which is a named tuple with three components:\n",
    "\n",
    "* `action` — the action to be taken (in this case, 0 or 1)\n",
    "* `state` — used for stateful (that is, RNN-based) policies\n",
    "* `info` — auxiliary data, such as log probabilities of actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "_bY8VfykPp8i"
   },
   "outputs": [],
   "source": [
    "example_environment = tf_py_environment.TFPyEnvironment(\n",
    "    suite_gym.load('MountainCar-v0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "AvZFyjsIPsVC"
   },
   "outputs": [],
   "source": [
    "time_step = example_environment.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FIyzGwmqG8ib",
    "outputId": "2b383173-2038-4436-d59a-92c7c0c02d25"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(\n",
       "{'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>,\n",
       " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
       " 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
       " 'observation': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.5830122,  0.       ]], dtype=float32)>})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mC9ZZNcpPtsG",
    "outputId": "f21a3ad9-5570-40af-e7f8-1afb3b6a931d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=())"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_policy.action(time_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.6 Metrics and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common metric used to evaluate a policy is the average return. The return is the sum of rewards obtained while running a policy in an environment for an episode. Several episodes are run, creating an average return.\n",
    "\n",
    "The following function computes the average return of a policy, given the policy, environment, and a number of episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "_N70kXwgPv_1"
   },
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "\n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "\n",
    "        time_step = environment.reset()\n",
    "        episode_return = 0.0\n",
    "\n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = environment.step(action_step.action)\n",
    "            episode_return += time_step.reward\n",
    "        total_return += episode_return\n",
    "\n",
    "    avg_return = total_return / num_episodes\n",
    "    return avg_return.numpy()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average return of a random policy can be computed, which gives -200.0. This will serve as a benchmark to check whether the agent is learning throughout the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Wbxlk2BPyLA",
    "outputId": "b027ec68-23ad-464a-b55e-394d65fb930f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-200.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_avg_return(eval_env, random_policy, num_eval_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.7 Replay Buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to keep track of the data collected from the environment, `Reverb` was also used, which is a replay system by Deepmind that only runs in Linux-based Operating Systems. Since this notebook was ran in Colab, it was also possible to `Reverb` there. The function of the replay buffer is to store experience data when collecting trajectories and is consumed during training. This replay buffer is constructed using specs describing the tensors that are to be stored, which can be obtained from the agent using agent.collect_data_spec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "_oNeEVJEP0XC"
   },
   "outputs": [],
   "source": [
    "table_name = 'uniform_table'\n",
    "replay_buffer_signature = tensor_spec.from_spec(\n",
    "      agent.collect_data_spec)\n",
    "replay_buffer_signature = tensor_spec.add_outer_dim(\n",
    "    replay_buffer_signature)\n",
    "\n",
    "table = reverb.Table(\n",
    "    table_name,\n",
    "    max_size=replay_buffer_max_length,\n",
    "    sampler=reverb.selectors.Uniform(),\n",
    "    remover=reverb.selectors.Fifo(),\n",
    "    rate_limiter=reverb.rate_limiters.MinSize(1),\n",
    "    signature=replay_buffer_signature)\n",
    "\n",
    "reverb_server = reverb.Server([table])\n",
    "\n",
    "replay_buffer = reverb_replay_buffer.ReverbReplayBuffer(\n",
    "    agent.collect_data_spec,\n",
    "    table_name=table_name,\n",
    "    sequence_length=2,\n",
    "    local_server=reverb_server)\n",
    "\n",
    "rb_observer = reverb_utils.ReverbAddTrajectoryObserver(\n",
    "    replay_buffer.py_client,\n",
    "    table_name,\n",
    "    sequence_length=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For most agents, `collect_data_spec` is a named tuple called Trajectory, containing the specs for observations, actions, rewards, and other items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wcuU1UD9P5Ke",
    "outputId": "5945eff6-8176-429e-b3d2-cca85ff0d59a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Trajectory(\n",
       "{'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n",
       " 'observation': BoundedTensorSpec(shape=(2,), dtype=tf.float32, name='observation', minimum=array([-1.2 , -0.07], dtype=float32), maximum=array([0.6 , 0.07], dtype=float32)),\n",
       " 'action': BoundedTensorSpec(shape=(), dtype=tf.int64, name='action', minimum=array(0), maximum=array(2)),\n",
       " 'policy_info': (),\n",
       " 'next_step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n",
       " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
       " 'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32))})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.collect_data_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s-6-CHYSP9Pr",
    "outputId": "6b23f802-4003-4af7-f16c-b95188066fc2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('step_type',\n",
       " 'observation',\n",
       " 'action',\n",
       " 'policy_info',\n",
       " 'next_step_type',\n",
       " 'reward',\n",
       " 'discount')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.collect_data_spec._fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.8 Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the random policy in the environment is executed for a few steps, recording the data in the replay buffer. Here, 'PyDriver' is used to run the experience collecting loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k5gTMooFQCvx",
    "outputId": "83da7201-1972-408c-c894-9eec74ded1b4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TimeStep(\n",
       " {'step_type': array(1, dtype=int32),\n",
       "  'reward': array(-1., dtype=float32),\n",
       "  'discount': array(1., dtype=float32),\n",
       "  'observation': array([-0.517807  , -0.00073633], dtype=float32)}),\n",
       " ())"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py_driver.PyDriver(\n",
    "    env,\n",
    "    py_tf_eager_policy.PyTFEagerPolicy(\n",
    "      random_policy, use_tf_function=True),\n",
    "    [rb_observer],\n",
    "    max_steps=initial_collect_steps).run(train_py_env.reset())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The replay buffer is now a collection of Trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CBfabuY-QX7r",
    "outputId": "6ad7dd26-0227-4e22-dd24-37de42080ac6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Trajectory(\n",
       " {'step_type': <tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 1], dtype=int32)>,\n",
       "  'observation': <tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       " array([[-5.1958227e-01, -5.3331786e-04],\n",
       "        [-5.1914573e-01,  4.3655929e-04]], dtype=float32)>,\n",
       "  'action': <tf.Tensor: shape=(2,), dtype=int64, numpy=array([2, 2])>,\n",
       "  'policy_info': (),\n",
       "  'next_step_type': <tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 1], dtype=int32)>,\n",
       "  'reward': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([-1., -1.], dtype=float32)>,\n",
       "  'discount': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 1.], dtype=float32)>}),\n",
       " SampleInfo(key=<tf.Tensor: shape=(2,), dtype=uint64, numpy=array([756609591310377412, 756609591310377412], dtype=uint64)>, probability=<tf.Tensor: shape=(2,), dtype=float64, numpy=array([0.01030928, 0.01030928])>, table_size=<tf.Tensor: shape=(2,), dtype=int64, numpy=array([97, 97])>, priority=<tf.Tensor: shape=(2,), dtype=float64, numpy=array([1., 1.])>, times_sampled=<tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 1], dtype=int32)>))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter(replay_buffer.as_dataset()).next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent needs access to the replay buffer. This is provided by creating an iterable tf.data.Dataset pipeline which will feed data to the agent. Each row of the replay buffer only stores a single observation step. But since the DQN Agent needs both the current and next observation to compute the loss, the dataset pipeline will sample two adjacent rows for each item in the batch (num_steps=2). This dataset is also optimized by running parallel calls and prefetching data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "od_oSFgMQb9O",
    "outputId": "616598d8-d6f8-4337-816d-1a0e6410c3a8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec=(Trajectory(\n",
       "{'step_type': TensorSpec(shape=(64, 2), dtype=tf.int32, name=None),\n",
       " 'observation': TensorSpec(shape=(64, 2, 2), dtype=tf.float32, name=None),\n",
       " 'action': TensorSpec(shape=(64, 2), dtype=tf.int64, name=None),\n",
       " 'policy_info': (),\n",
       " 'next_step_type': TensorSpec(shape=(64, 2), dtype=tf.int32, name=None),\n",
       " 'reward': TensorSpec(shape=(64, 2), dtype=tf.float32, name=None),\n",
       " 'discount': TensorSpec(shape=(64, 2), dtype=tf.float32, name=None)}), SampleInfo(key=TensorSpec(shape=(64, 2), dtype=tf.uint64, name=None), probability=TensorSpec(shape=(64, 2), dtype=tf.float64, name=None), table_size=TensorSpec(shape=(64, 2), dtype=tf.int64, name=None), priority=TensorSpec(shape=(64, 2), dtype=tf.float64, name=None), times_sampled=TensorSpec(shape=(64, 2), dtype=tf.int32, name=None)))>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset generates trajectories with shape [Bx2x...]\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3,\n",
    "    sample_batch_size=batch_size,\n",
    "    num_steps=2).prefetch(3)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "dOwShxVVQc9I"
   },
   "outputs": [],
   "source": [
    "iterator = iter(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.9 Training the Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two things must happen during the training loop:\n",
    "\n",
    "* collect data from the environment\n",
    "* use that data to train the agent's neural network(s)\n",
    "\n",
    "The policy is also periodically evaluated and the current score printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CGiGURnPQnQy",
    "outputId": "726d08bf-0582-4226-93be-96de35f4a2f8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1260: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.foldr(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 200: loss = 0.14048108458518982\n",
      "step = 400: loss = 0.0074961064383387566\n",
      "step = 600: loss = 0.012236947193741798\n",
      "step = 800: loss = 0.01514175534248352\n",
      "step = 1000: loss = 0.02781202457845211\n",
      "step = 1000: Average Return = -200.0\n",
      "step = 1200: loss = 0.012369811534881592\n",
      "step = 1400: loss = 0.009981459006667137\n",
      "step = 1600: loss = 0.017119042575359344\n",
      "step = 1800: loss = 0.010994571261107922\n",
      "step = 2000: loss = 0.011184653267264366\n",
      "step = 2000: Average Return = -200.0\n",
      "step = 2200: loss = 0.03767845034599304\n",
      "step = 2400: loss = 0.07815498113632202\n",
      "step = 2600: loss = 0.009657658636569977\n",
      "step = 2800: loss = 0.006783921737223864\n",
      "step = 3000: loss = 0.006777910050004721\n",
      "step = 3000: Average Return = -200.0\n",
      "step = 3200: loss = 0.0038203070871531963\n",
      "step = 3400: loss = 0.004134439863264561\n",
      "step = 3600: loss = 0.004961574450135231\n",
      "step = 3800: loss = 0.005959300324320793\n",
      "step = 4000: loss = 0.002890238305553794\n",
      "step = 4000: Average Return = -200.0\n",
      "step = 4200: loss = 0.004730669781565666\n",
      "step = 4400: loss = 0.007512026932090521\n",
      "step = 4600: loss = 0.07690032571554184\n",
      "step = 4800: loss = 0.0023822886869311333\n",
      "step = 5000: loss = 0.11246839910745621\n",
      "step = 5000: Average Return = -200.0\n",
      "step = 5200: loss = 0.004014221951365471\n",
      "step = 5400: loss = 0.0041039627976715565\n",
      "step = 5600: loss = 0.34199637174606323\n",
      "step = 5800: loss = 0.002595355734229088\n",
      "step = 6000: loss = 0.20379950106143951\n",
      "step = 6000: Average Return = -200.0\n",
      "step = 6200: loss = 0.21594254672527313\n",
      "step = 6400: loss = 0.003539502387866378\n",
      "step = 6600: loss = 0.22884052991867065\n",
      "step = 6800: loss = 0.0023279774468392134\n",
      "step = 7000: loss = 0.0027329400181770325\n",
      "step = 7000: Average Return = -200.0\n",
      "step = 7200: loss = 0.0018640339840203524\n",
      "step = 7400: loss = 0.0032952723558992147\n",
      "step = 7600: loss = 0.0017758049070835114\n",
      "step = 7800: loss = 0.005326126702129841\n",
      "step = 8000: loss = 0.3263382315635681\n",
      "step = 8000: Average Return = -200.0\n",
      "step = 8200: loss = 0.3530045747756958\n",
      "step = 8400: loss = 0.0015575157012790442\n",
      "step = 8600: loss = 0.0014122528955340385\n",
      "step = 8800: loss = 0.004019985906779766\n",
      "step = 9000: loss = 0.0018157345475628972\n",
      "step = 9000: Average Return = -200.0\n",
      "step = 9200: loss = 0.33107656240463257\n",
      "step = 9400: loss = 0.001687084324657917\n",
      "step = 9600: loss = 0.40188851952552795\n",
      "step = 9800: loss = 0.0010417650919407606\n",
      "step = 10000: loss = 0.4047849178314209\n",
      "step = 10000: Average Return = -200.0\n",
      "step = 10200: loss = 0.0014839386567473412\n",
      "step = 10400: loss = 0.002704199403524399\n",
      "step = 10600: loss = 0.001178358099423349\n",
      "step = 10800: loss = 0.0025586974807083607\n",
      "step = 11000: loss = 0.002399969846010208\n",
      "step = 11000: Average Return = -200.0\n",
      "step = 11200: loss = 0.49280574917793274\n",
      "step = 11400: loss = 0.5249491930007935\n",
      "step = 11600: loss = 0.0014669743832200766\n",
      "step = 11800: loss = 0.0015372876077890396\n",
      "step = 12000: loss = 0.5228471755981445\n",
      "step = 12000: Average Return = -200.0\n",
      "step = 12200: loss = 0.0024564380291849375\n",
      "step = 12400: loss = 0.007853946648538113\n",
      "step = 12600: loss = 0.635199248790741\n",
      "step = 12800: loss = 0.0008933336939662695\n",
      "step = 13000: loss = 0.0012244778918102384\n",
      "step = 13000: Average Return = -200.0\n",
      "step = 13200: loss = 0.0015959164593368769\n",
      "step = 13400: loss = 0.0009336781222373247\n",
      "step = 13600: loss = 0.0041283853352069855\n",
      "step = 13800: loss = 0.010408012196421623\n",
      "step = 14000: loss = 0.7034316658973694\n",
      "step = 14000: Average Return = -200.0\n",
      "step = 14200: loss = 0.001193774864077568\n",
      "step = 14400: loss = 0.00048463125131092966\n",
      "step = 14600: loss = 0.0011262116022408009\n",
      "step = 14800: loss = 0.002191111445426941\n",
      "step = 15000: loss = 0.6460138559341431\n",
      "step = 15000: Average Return = -200.0\n",
      "step = 15200: loss = 0.0007838418823666871\n",
      "step = 15400: loss = 0.001217791810631752\n",
      "step = 15600: loss = 0.00314030097797513\n",
      "step = 15800: loss = 0.6230911612510681\n",
      "step = 16000: loss = 0.0007474493468180299\n",
      "step = 16000: Average Return = -200.0\n",
      "step = 16200: loss = 0.001468659145757556\n",
      "step = 16400: loss = 0.003399093635380268\n",
      "step = 16600: loss = 0.7557435035705566\n",
      "step = 16800: loss = 0.6569933891296387\n",
      "step = 17000: loss = 0.0017634446267038584\n",
      "step = 17000: Average Return = -200.0\n",
      "step = 17200: loss = 0.7409305572509766\n",
      "step = 17400: loss = 0.0019732718355953693\n",
      "step = 17600: loss = 0.00064990104874596\n",
      "step = 17800: loss = 1.580754280090332\n",
      "step = 18000: loss = 0.7647377252578735\n",
      "step = 18000: Average Return = -200.0\n",
      "step = 18200: loss = 0.753505289554596\n",
      "step = 18400: loss = 0.7537149786949158\n",
      "step = 18600: loss = 0.0005765481037087739\n",
      "step = 18800: loss = 0.0021418631076812744\n",
      "step = 19000: loss = 0.0010201934492215514\n",
      "step = 19000: Average Return = -200.0\n",
      "step = 19200: loss = 0.0006363018183037639\n",
      "step = 19400: loss = 0.7944104671478271\n",
      "step = 19600: loss = 0.0034432762768119574\n",
      "step = 19800: loss = 0.0002488483733031899\n",
      "step = 20000: loss = 0.8463361263275146\n",
      "step = 20000: Average Return = -200.0\n",
      "step = 20200: loss = 0.0025709487963467836\n",
      "step = 20400: loss = 0.0016900955233722925\n",
      "step = 20600: loss = 0.0004479294002521783\n",
      "step = 20800: loss = 0.0008604404865764081\n",
      "step = 21000: loss = 0.0027450709603726864\n",
      "step = 21000: Average Return = -200.0\n",
      "step = 21200: loss = 0.004527236334979534\n",
      "step = 21400: loss = 0.7601125240325928\n",
      "step = 21600: loss = 0.8403620719909668\n",
      "step = 21800: loss = 0.005412232130765915\n",
      "step = 22000: loss = 0.0007489493582397699\n",
      "step = 22000: Average Return = -200.0\n",
      "step = 22200: loss = 0.0011013549519702792\n",
      "step = 22400: loss = 0.8876633644104004\n",
      "step = 22600: loss = 0.8893976211547852\n",
      "step = 22800: loss = 0.0032726023346185684\n",
      "step = 23000: loss = 0.005512921139597893\n",
      "step = 23000: Average Return = -200.0\n",
      "step = 23200: loss = 0.010158780962228775\n",
      "step = 23400: loss = 0.00445817643776536\n",
      "step = 23600: loss = 0.9703450202941895\n",
      "step = 23800: loss = 0.0025637936778366566\n",
      "step = 24000: loss = 0.0032266550697386265\n",
      "step = 24000: Average Return = -200.0\n",
      "step = 24200: loss = 0.0030516546685248613\n",
      "step = 24400: loss = 0.006190501153469086\n",
      "step = 24600: loss = 1.854379415512085\n",
      "step = 24800: loss = 0.0024697529152035713\n",
      "step = 25000: loss = 1.0120515823364258\n",
      "step = 25000: Average Return = -200.0\n",
      "step = 25200: loss = 0.0005385861732065678\n",
      "step = 25400: loss = 0.0043157655745744705\n",
      "step = 25600: loss = 0.002826880430802703\n",
      "step = 25800: loss = 0.00188748212531209\n",
      "step = 26000: loss = 0.00336654856801033\n",
      "step = 26000: Average Return = -200.0\n",
      "step = 26200: loss = 0.0006270556477829814\n",
      "step = 26400: loss = 0.0019358485005795956\n",
      "step = 26600: loss = 0.0014678873121738434\n",
      "step = 26800: loss = 0.0058954074047505856\n",
      "step = 27000: loss = 0.0028859246522188187\n",
      "step = 27000: Average Return = -200.0\n",
      "step = 27200: loss = 0.0003854131791740656\n",
      "step = 27400: loss = 0.001604154473170638\n",
      "step = 27600: loss = 0.000497768516652286\n",
      "step = 27800: loss = 0.0012124914210289717\n",
      "step = 28000: loss = 0.005049337632954121\n",
      "step = 28000: Average Return = -200.0\n",
      "step = 28200: loss = 0.0035712188109755516\n",
      "step = 28400: loss = 0.0020884457044303417\n",
      "step = 28600: loss = 0.0020927670411765575\n",
      "step = 28800: loss = 0.0005088798934593797\n",
      "step = 29000: loss = 0.005431449040770531\n",
      "step = 29000: Average Return = -200.0\n",
      "step = 29200: loss = 0.0024607153609395027\n",
      "step = 29400: loss = 3.0086894035339355\n",
      "step = 29600: loss = 0.0010291344951838255\n",
      "step = 29800: loss = 0.0021289545111358166\n",
      "step = 30000: loss = 3.0328285694122314\n",
      "step = 30000: Average Return = -200.0\n",
      "step = 30200: loss = 0.0021612236741930246\n",
      "step = 30400: loss = 0.002804783172905445\n",
      "step = 30600: loss = 0.0009466285118833184\n",
      "step = 30800: loss = 0.004078461788594723\n",
      "step = 31000: loss = 0.002434526337310672\n",
      "step = 31000: Average Return = -200.0\n",
      "step = 31200: loss = 0.002102768514305353\n",
      "step = 31400: loss = 1.021422266960144\n",
      "step = 31600: loss = 1.0521061420440674\n",
      "step = 31800: loss = 0.004116118419915438\n",
      "step = 32000: loss = 0.0007741578738205135\n",
      "step = 32000: Average Return = -200.0\n",
      "step = 32200: loss = 0.0016337439883500338\n",
      "step = 32400: loss = 0.002397283911705017\n",
      "step = 32600: loss = 0.000655328098218888\n",
      "step = 32800: loss = 0.0004966009291820228\n",
      "step = 33000: loss = 0.002707436215132475\n",
      "step = 33000: Average Return = -200.0\n",
      "step = 33200: loss = 0.005052283871918917\n",
      "step = 33400: loss = 0.003255885560065508\n",
      "step = 33600: loss = 1.0794084072113037\n",
      "step = 33800: loss = 1.0686973333358765\n",
      "step = 34000: loss = 0.0007435043808072805\n",
      "step = 34000: Average Return = -200.0\n",
      "step = 34200: loss = 0.003803734201937914\n",
      "step = 34400: loss = 1.0392041206359863\n",
      "step = 34600: loss = 0.017219562083482742\n",
      "step = 34800: loss = 1.0564895868301392\n",
      "step = 35000: loss = 0.0022571622394025326\n",
      "step = 35000: Average Return = -200.0\n",
      "step = 35200: loss = 0.0006631508003920317\n",
      "step = 35400: loss = 0.0014192659873515368\n",
      "step = 35600: loss = 1.081884503364563\n",
      "step = 35800: loss = 0.004763204604387283\n",
      "step = 36000: loss = 1.0737022161483765\n",
      "step = 36000: Average Return = -200.0\n",
      "step = 36200: loss = 0.0046265944838523865\n",
      "step = 36400: loss = 0.0005333487060852349\n",
      "step = 36600: loss = 0.0025217351503670216\n",
      "step = 36800: loss = 1.051887035369873\n",
      "step = 37000: loss = 0.005052943713963032\n",
      "step = 37000: Average Return = -200.0\n",
      "step = 37200: loss = 0.0031736306846141815\n",
      "step = 37400: loss = 0.0017522824928164482\n",
      "step = 37600: loss = 1.0925625562667847\n",
      "step = 37800: loss = 2.1693387031555176\n",
      "step = 38000: loss = 0.0034425503108650446\n",
      "step = 38000: Average Return = -200.0\n",
      "step = 38200: loss = 0.014246964827179909\n",
      "step = 38400: loss = 0.007622172590345144\n",
      "step = 38600: loss = 0.004262212198227644\n",
      "step = 38800: loss = 0.006161231081932783\n",
      "step = 39000: loss = 0.004840870387852192\n",
      "step = 39000: Average Return = -200.0\n",
      "step = 39200: loss = 0.9731230735778809\n",
      "step = 39400: loss = 0.0007951162406243384\n",
      "step = 39600: loss = 1.087917685508728\n",
      "step = 39800: loss = 0.003159154672175646\n",
      "step = 40000: loss = 0.000876627629622817\n",
      "step = 40000: Average Return = -200.0\n",
      "step = 40200: loss = 0.0013435035943984985\n",
      "step = 40400: loss = 0.003473934717476368\n",
      "step = 40600: loss = 0.0003833325463347137\n",
      "step = 40800: loss = 0.0050078099593520164\n",
      "step = 41000: loss = 0.003801127430051565\n",
      "step = 41000: Average Return = -200.0\n",
      "step = 41200: loss = 0.000621273647993803\n",
      "step = 41400: loss = 0.0018525682389736176\n",
      "step = 41600: loss = 0.0028405305929481983\n",
      "step = 41800: loss = 0.002821518573909998\n",
      "step = 42000: loss = 1.0433589220046997\n",
      "step = 42000: Average Return = -200.0\n",
      "step = 42200: loss = 1.1382421255111694\n",
      "step = 42400: loss = 1.067460298538208\n",
      "step = 42600: loss = 1.1302242279052734\n",
      "step = 42800: loss = 1.0453574657440186\n",
      "step = 43000: loss = 0.0036839034873992205\n",
      "step = 43000: Average Return = -200.0\n",
      "step = 43200: loss = 1.0410186052322388\n",
      "step = 43400: loss = 0.0020181220024824142\n",
      "step = 43600: loss = 0.9487289786338806\n",
      "step = 43800: loss = 0.010785205289721489\n",
      "step = 44000: loss = 0.0023822407238185406\n",
      "step = 44000: Average Return = -200.0\n",
      "step = 44200: loss = 1.086761236190796\n",
      "step = 44400: loss = 0.0005255383439362049\n",
      "step = 44600: loss = 0.0012896201806142926\n",
      "step = 44800: loss = 0.003934046253561974\n",
      "step = 45000: loss = 1.1246525049209595\n",
      "step = 45000: Average Return = -200.0\n",
      "step = 45200: loss = 0.004315251484513283\n",
      "step = 45400: loss = 2.267408847808838\n",
      "step = 45600: loss = 2.1919798851013184\n",
      "step = 45800: loss = 0.004763671196997166\n",
      "step = 46000: loss = 0.003726538736373186\n",
      "step = 46000: Average Return = -200.0\n",
      "step = 46200: loss = 0.0034688066225498915\n",
      "step = 46400: loss = 0.006958916783332825\n",
      "step = 46600: loss = 2.0882680416107178\n",
      "step = 46800: loss = 0.007472286932170391\n",
      "step = 47000: loss = 1.0838261842727661\n",
      "step = 47000: Average Return = -200.0\n",
      "step = 47200: loss = 2.157599687576294\n",
      "step = 47400: loss = 0.004193579778075218\n",
      "step = 47600: loss = 0.0008687310619279742\n",
      "step = 47800: loss = 0.0004765232733916491\n",
      "step = 48000: loss = 0.0011969658080488443\n",
      "step = 48000: Average Return = -200.0\n",
      "step = 48200: loss = 0.001644597388803959\n",
      "step = 48400: loss = 0.0036854331847280264\n",
      "step = 48600: loss = 1.045707106590271\n",
      "step = 48800: loss = 0.0006254192558117211\n",
      "step = 49000: loss = 1.0928504467010498\n",
      "step = 49000: Average Return = -200.0\n",
      "step = 49200: loss = 1.089859127998352\n",
      "step = 49400: loss = 0.00126159458886832\n",
      "step = 49600: loss = 0.004286499693989754\n",
      "step = 49800: loss = 0.007942444644868374\n",
      "step = 50000: loss = 1.069795846939087\n",
      "step = 50000: Average Return = -200.0\n",
      "step = 50200: loss = 0.002011593198403716\n",
      "step = 50400: loss = 0.00113729911390692\n",
      "step = 50600: loss = 1.104317307472229\n",
      "step = 50800: loss = 1.0572007894515991\n",
      "step = 51000: loss = 0.0007917545735836029\n",
      "step = 51000: Average Return = -200.0\n",
      "step = 51200: loss = 0.0031995344907045364\n",
      "step = 51400: loss = 2.286550283432007\n",
      "step = 51600: loss = 0.0010551969753578305\n",
      "step = 51800: loss = 0.002444421174004674\n",
      "step = 52000: loss = 1.1335591077804565\n",
      "step = 52000: Average Return = -200.0\n",
      "step = 52200: loss = 0.0042291367426514626\n",
      "step = 52400: loss = 0.0022833477705717087\n",
      "step = 52600: loss = 2.2782866954803467\n",
      "step = 52800: loss = 1.1508336067199707\n",
      "step = 53000: loss = 0.007934052497148514\n",
      "step = 53000: Average Return = -200.0\n",
      "step = 53200: loss = 0.0051740859635174274\n",
      "step = 53400: loss = 0.00508872140198946\n",
      "step = 53600: loss = 0.0024393603671342134\n",
      "step = 53800: loss = 0.0017453362233936787\n",
      "step = 54000: loss = 1.094198226928711\n",
      "step = 54000: Average Return = -200.0\n",
      "step = 54200: loss = 0.0030814316123723984\n",
      "step = 54400: loss = 1.0315866470336914\n",
      "step = 54600: loss = 0.0023792055435478687\n",
      "step = 54800: loss = 0.006131843663752079\n",
      "step = 55000: loss = 0.0033328174613416195\n",
      "step = 55000: Average Return = -200.0\n",
      "step = 55200: loss = 0.002720980439335108\n",
      "step = 55400: loss = 0.0036855991929769516\n",
      "step = 55600: loss = 0.004639402497559786\n",
      "step = 55800: loss = 0.003121469635516405\n",
      "step = 56000: loss = 1.0797110795974731\n",
      "step = 56000: Average Return = -200.0\n",
      "step = 56200: loss = 0.0007924576057121158\n",
      "step = 56400: loss = 0.008710742928087711\n",
      "step = 56600: loss = 0.0004900620551779866\n",
      "step = 56800: loss = 0.0035660150460898876\n",
      "step = 57000: loss = 0.0017194157699123025\n",
      "step = 57000: Average Return = -200.0\n",
      "step = 57200: loss = 0.0005132789956405759\n",
      "step = 57400: loss = 0.003154831938445568\n",
      "step = 57600: loss = 0.007949747145175934\n",
      "step = 57800: loss = 0.0007017171010375023\n",
      "step = 58000: loss = 0.0027243318036198616\n",
      "step = 58000: Average Return = -200.0\n",
      "step = 58200: loss = 2.2426581382751465\n",
      "step = 58400: loss = 1.0636719465255737\n",
      "step = 58600: loss = 1.0793445110321045\n",
      "step = 58800: loss = 0.0015666921390220523\n",
      "step = 59000: loss = 0.0010335012339055538\n",
      "step = 59000: Average Return = -200.0\n",
      "step = 59200: loss = 0.0004980656085535884\n",
      "step = 59400: loss = 0.0038806709926575422\n",
      "step = 59600: loss = 0.0003603976801969111\n",
      "step = 59800: loss = 1.018028736114502\n",
      "step = 60000: loss = 0.0011267532827332616\n",
      "step = 60000: Average Return = -200.0\n",
      "step = 60200: loss = 0.006914936937391758\n",
      "step = 60400: loss = 0.0036980826407670975\n",
      "step = 60600: loss = 2.209989547729492\n",
      "step = 60800: loss = 1.0085053443908691\n",
      "step = 61000: loss = 2.19856333732605\n",
      "step = 61000: Average Return = -200.0\n",
      "step = 61200: loss = 0.0028945745434612036\n",
      "step = 61400: loss = 0.001559136901050806\n",
      "step = 61600: loss = 0.0016166272107511759\n",
      "step = 61800: loss = 0.005289943423122168\n",
      "step = 62000: loss = 1.1330246925354004\n",
      "step = 62000: Average Return = -200.0\n",
      "step = 62200: loss = 1.0918073654174805\n",
      "step = 62400: loss = 0.0030218607280403376\n",
      "step = 62600: loss = 0.002214414067566395\n",
      "step = 62800: loss = 1.0897361040115356\n",
      "step = 63000: loss = 0.0010538455098867416\n",
      "step = 63000: Average Return = -200.0\n",
      "step = 63200: loss = 0.006734704133123159\n",
      "step = 63400: loss = 0.0006648583803325891\n",
      "step = 63600: loss = 0.0045900773257017136\n",
      "step = 63800: loss = 0.0016865483485162258\n",
      "step = 64000: loss = 0.0009362612036056817\n",
      "step = 64000: Average Return = -200.0\n",
      "step = 64200: loss = 0.00150249432772398\n",
      "step = 64400: loss = 0.0015507080825045705\n",
      "step = 64600: loss = 1.1275156736373901\n",
      "step = 64800: loss = 0.0014657526044175029\n",
      "step = 65000: loss = 0.0009502177708782256\n",
      "step = 65000: Average Return = -200.0\n",
      "step = 65200: loss = 0.0012074935948476195\n",
      "step = 65400: loss = 1.1530545949935913\n",
      "step = 65600: loss = 0.005809222348034382\n",
      "step = 65800: loss = 0.006865723989903927\n",
      "step = 66000: loss = 0.0008547783363610506\n",
      "step = 66000: Average Return = -200.0\n",
      "step = 66200: loss = 1.122825264930725\n",
      "step = 66400: loss = 0.002150430576875806\n",
      "step = 66600: loss = 0.010868558660149574\n",
      "step = 66800: loss = 1.1131192445755005\n",
      "step = 67000: loss = 0.005907606333494186\n",
      "step = 67000: Average Return = -200.0\n",
      "step = 67200: loss = 0.007403075695037842\n",
      "step = 67400: loss = 1.1135077476501465\n",
      "step = 67600: loss = 0.003988868556916714\n",
      "step = 67800: loss = 0.001815855037420988\n",
      "step = 68000: loss = 0.001586856204085052\n",
      "step = 68000: Average Return = -200.0\n",
      "step = 68200: loss = 0.0018978817388415337\n",
      "step = 68400: loss = 1.1481777429580688\n",
      "step = 68600: loss = 0.0021930215880274773\n",
      "step = 68800: loss = 2.1827633380889893\n",
      "step = 69000: loss = 0.004032234661281109\n",
      "step = 69000: Average Return = -200.0\n",
      "step = 69200: loss = 0.0014898377703502774\n",
      "step = 69400: loss = 0.001923066796734929\n",
      "step = 69600: loss = 1.1509612798690796\n",
      "step = 69800: loss = 0.002804623916745186\n",
      "step = 70000: loss = 0.00876213051378727\n",
      "step = 70000: Average Return = -200.0\n",
      "step = 70200: loss = 0.004075727425515652\n",
      "step = 70400: loss = 0.0019286132883280516\n",
      "step = 70600: loss = 0.0010861098999157548\n",
      "step = 70800: loss = 0.014256885275244713\n",
      "step = 71000: loss = 0.005108586046844721\n",
      "step = 71000: Average Return = -200.0\n",
      "step = 71200: loss = 0.005651175510138273\n",
      "step = 71400: loss = 1.1122571229934692\n",
      "step = 71600: loss = 0.0048690252006053925\n",
      "step = 71800: loss = 0.0003717851941473782\n",
      "step = 72000: loss = 0.001976602477952838\n",
      "step = 72000: Average Return = -200.0\n",
      "step = 72200: loss = 0.0007949650753289461\n",
      "step = 72400: loss = 0.0008349737036041915\n",
      "step = 72600: loss = 0.7267543077468872\n",
      "step = 72800: loss = 0.0036709392443299294\n",
      "step = 73000: loss = 0.010454753413796425\n",
      "step = 73000: Average Return = -200.0\n",
      "step = 73200: loss = 0.004995879717171192\n",
      "step = 73400: loss = 0.002688358072191477\n",
      "step = 73600: loss = 0.005934952758252621\n",
      "step = 73800: loss = 0.5354471802711487\n",
      "step = 74000: loss = 0.004417854826897383\n",
      "step = 74000: Average Return = -200.0\n",
      "step = 74200: loss = 0.005310787819325924\n",
      "step = 74400: loss = 0.0020270440727472305\n",
      "step = 74600: loss = 0.0049921320751309395\n",
      "step = 74800: loss = 0.0028898639138787985\n",
      "step = 75000: loss = 0.011261027306318283\n",
      "step = 75000: Average Return = -200.0\n",
      "step = 75200: loss = 1.1634241342544556\n",
      "step = 75400: loss = 0.003977623302489519\n",
      "step = 75600: loss = 0.004711279645562172\n",
      "step = 75800: loss = 0.0019389516673982143\n",
      "step = 76000: loss = 0.0006028959760442376\n",
      "step = 76000: Average Return = -200.0\n",
      "step = 76200: loss = 1.1406477689743042\n",
      "step = 76400: loss = 1.17192542552948\n",
      "step = 76600: loss = 0.0042350515723228455\n",
      "step = 76800: loss = 0.0008543313015252352\n",
      "step = 77000: loss = 1.1650625467300415\n",
      "step = 77000: Average Return = -200.0\n",
      "step = 77200: loss = 0.006599628832191229\n",
      "step = 77400: loss = 0.0007613931084051728\n",
      "step = 77600: loss = 0.004239378497004509\n",
      "step = 77800: loss = 0.003418518230319023\n",
      "step = 78000: loss = 0.0021345089189708233\n",
      "step = 78000: Average Return = -200.0\n",
      "step = 78200: loss = 1.1599705219268799\n",
      "step = 78400: loss = 2.2366175651550293\n",
      "step = 78600: loss = 0.001114017330110073\n",
      "step = 78800: loss = 0.0021287021227180958\n",
      "step = 79000: loss = 0.0027157189324498177\n",
      "step = 79000: Average Return = -200.0\n",
      "step = 79200: loss = 0.002133715432137251\n",
      "step = 79400: loss = 0.00415377039462328\n",
      "step = 79600: loss = 0.0013549800496548414\n",
      "step = 79800: loss = 0.0008286999072879553\n",
      "step = 80000: loss = 0.005274603143334389\n",
      "step = 80000: Average Return = -200.0\n",
      "step = 80200: loss = 0.0022634435445070267\n",
      "step = 80400: loss = 0.006288849748671055\n",
      "step = 80600: loss = 0.0023122497368603945\n",
      "step = 80800: loss = 1.1346776485443115\n",
      "step = 81000: loss = 0.9972445368766785\n",
      "step = 81000: Average Return = -200.0\n",
      "step = 81200: loss = 0.005369719583541155\n",
      "step = 81400: loss = 0.0015607476234436035\n",
      "step = 81600: loss = 2.055004119873047\n",
      "step = 81800: loss = 0.004178939387202263\n",
      "step = 82000: loss = 0.006163646467030048\n",
      "step = 82000: Average Return = -200.0\n",
      "step = 82200: loss = 0.002353568095713854\n",
      "step = 82400: loss = 1.1309157609939575\n",
      "step = 82600: loss = 1.095066785812378\n",
      "step = 82800: loss = 0.000891174073331058\n",
      "step = 83000: loss = 0.0010538813658058643\n",
      "step = 83000: Average Return = -200.0\n",
      "step = 83200: loss = 0.006369912996888161\n",
      "step = 83400: loss = 1.1670844554901123\n",
      "step = 83600: loss = 0.0016409712843596935\n",
      "step = 83800: loss = 1.1860605478286743\n",
      "step = 84000: loss = 0.0021608194801956415\n",
      "step = 84000: Average Return = -200.0\n",
      "step = 84200: loss = 1.1262909173965454\n",
      "step = 84400: loss = 0.006285727955400944\n",
      "step = 84600: loss = 1.0598204135894775\n",
      "step = 84800: loss = 1.0208101272583008\n",
      "step = 85000: loss = 0.007400606758892536\n",
      "step = 85000: Average Return = -200.0\n",
      "step = 85200: loss = 0.001384946284815669\n",
      "step = 85400: loss = 0.008030988276004791\n",
      "step = 85600: loss = 1.0339964628219604\n",
      "step = 85800: loss = 0.0042009176686406136\n",
      "step = 86000: loss = 0.004616561345756054\n",
      "step = 86000: Average Return = -200.0\n",
      "step = 86200: loss = 0.989421010017395\n",
      "step = 86400: loss = 0.00600289274007082\n",
      "step = 86600: loss = 0.0020078227389603853\n",
      "step = 86800: loss = 0.003827849868685007\n",
      "step = 87000: loss = 0.007103052455931902\n",
      "step = 87000: Average Return = -200.0\n",
      "step = 87200: loss = 0.000909863505512476\n",
      "step = 87400: loss = 1.1514983177185059\n",
      "step = 87600: loss = 0.003817404620349407\n",
      "step = 87800: loss = 1.1104894876480103\n",
      "step = 88000: loss = 0.00487632118165493\n",
      "step = 88000: Average Return = -200.0\n",
      "step = 88200: loss = 0.007815910503268242\n",
      "step = 88400: loss = 0.002002174500375986\n",
      "step = 88600: loss = 0.001148186158388853\n",
      "step = 88800: loss = 0.005401176866143942\n",
      "step = 89000: loss = 0.008280414156615734\n",
      "step = 89000: Average Return = -200.0\n",
      "step = 89200: loss = 0.0033262232318520546\n",
      "step = 89400: loss = 0.007679759059101343\n",
      "step = 89600: loss = 0.002020773012191057\n",
      "step = 89800: loss = 0.0006315470673143864\n",
      "step = 90000: loss = 0.0026443293318152428\n",
      "step = 90000: Average Return = -200.0\n",
      "step = 90200: loss = 0.0029618956614285707\n",
      "step = 90400: loss = 1.1256924867630005\n",
      "step = 90600: loss = 0.0014041655231267214\n",
      "step = 90800: loss = 0.0018900802824646235\n",
      "step = 91000: loss = 0.00848224014043808\n",
      "step = 91000: Average Return = -200.0\n",
      "step = 91200: loss = 0.0007587114814668894\n",
      "step = 91400: loss = 3.4046854972839355\n",
      "step = 91600: loss = 0.0010941583896055818\n",
      "step = 91800: loss = 0.005160581320524216\n",
      "step = 92000: loss = 0.0031665898859500885\n",
      "step = 92000: Average Return = -200.0\n",
      "step = 92200: loss = 0.00041234653326682746\n",
      "step = 92400: loss = 0.0010155548807233572\n",
      "step = 92600: loss = 0.0028826682828366756\n",
      "step = 92800: loss = 0.9160134792327881\n",
      "step = 93000: loss = 0.0028233686462044716\n",
      "step = 93000: Average Return = -200.0\n",
      "step = 93200: loss = 1.159743070602417\n",
      "step = 93400: loss = 0.0017788219265639782\n",
      "step = 93600: loss = 1.1399897336959839\n",
      "step = 93800: loss = 0.8343061208724976\n",
      "step = 94000: loss = 0.001153277000412345\n",
      "step = 94000: Average Return = -200.0\n",
      "step = 94200: loss = 1.0145654678344727\n",
      "step = 94400: loss = 0.0015572388656437397\n",
      "step = 94600: loss = 1.176438808441162\n",
      "step = 94800: loss = 0.0013122649397701025\n",
      "step = 95000: loss = 0.0018065990880131721\n",
      "step = 95000: Average Return = -200.0\n",
      "step = 95200: loss = 2.1808581352233887\n",
      "step = 95400: loss = 0.005520801059901714\n",
      "step = 95600: loss = 0.007061193697154522\n",
      "step = 95800: loss = 0.010756690055131912\n",
      "step = 96000: loss = 2.1773693561553955\n",
      "step = 96000: Average Return = -200.0\n",
      "step = 96200: loss = 0.001260337419807911\n",
      "step = 96400: loss = 0.004696402698755264\n",
      "step = 96600: loss = 0.0021934309042990208\n",
      "step = 96800: loss = 1.0782972574234009\n",
      "step = 97000: loss = 0.001988898031413555\n",
      "step = 97000: Average Return = -200.0\n",
      "step = 97200: loss = 2.123270034790039\n",
      "step = 97400: loss = 0.0031317630782723427\n",
      "step = 97600: loss = 0.0016119075007736683\n",
      "step = 97800: loss = 0.0007899866905063391\n",
      "step = 98000: loss = 0.005586313083767891\n",
      "step = 98000: Average Return = -200.0\n",
      "step = 98200: loss = 0.0051518818363547325\n",
      "step = 98400: loss = 0.0028844045009464025\n",
      "step = 98600: loss = 0.011851225048303604\n",
      "step = 98800: loss = 0.0017379538621753454\n",
      "step = 99000: loss = 0.014223529025912285\n",
      "step = 99000: Average Return = -200.0\n",
      "step = 99200: loss = 0.004727389197796583\n",
      "step = 99400: loss = 0.0009712285245768726\n",
      "step = 99600: loss = 1.1095679998397827\n",
      "step = 99800: loss = 0.0015694875037297606\n",
      "step = 100000: loss = 0.0016333303647115827\n",
      "step = 100000: Average Return = -200.0\n",
      "step = 100200: loss = 0.0019766262266784906\n",
      "step = 100400: loss = 0.002880276646465063\n",
      "step = 100600: loss = 1.0603281259536743\n",
      "step = 100800: loss = 1.1535412073135376\n",
      "step = 101000: loss = 0.001274876412935555\n",
      "step = 101000: Average Return = -200.0\n",
      "step = 101200: loss = 1.1031497716903687\n",
      "step = 101400: loss = 0.003819851903244853\n",
      "step = 101600: loss = 0.009000109508633614\n",
      "step = 101800: loss = 2.2469639778137207\n",
      "step = 102000: loss = 0.005705938674509525\n",
      "step = 102000: Average Return = -200.0\n",
      "step = 102200: loss = 1.1745426654815674\n",
      "step = 102400: loss = 1.166783332824707\n",
      "step = 102600: loss = 0.006981829646974802\n",
      "step = 102800: loss = 0.006148243322968483\n",
      "step = 103000: loss = 0.0008428142755292356\n",
      "step = 103000: Average Return = -200.0\n",
      "step = 103200: loss = 0.006433072965592146\n",
      "step = 103400: loss = 0.00028416275745257735\n",
      "step = 103600: loss = 0.003245818428695202\n",
      "step = 103800: loss = 1.1278644800186157\n",
      "step = 104000: loss = 1.143746256828308\n",
      "step = 104000: Average Return = -200.0\n",
      "step = 104200: loss = 0.007397732697427273\n",
      "step = 104400: loss = 0.005041301250457764\n",
      "step = 104600: loss = 1.1055691242218018\n",
      "step = 104800: loss = 0.0014361259527504444\n",
      "step = 105000: loss = 1.0685404539108276\n",
      "step = 105000: Average Return = -200.0\n",
      "step = 105200: loss = 0.0011100016999989748\n",
      "step = 105400: loss = 0.00101746735163033\n",
      "step = 105600: loss = 0.0032609482295811176\n",
      "step = 105800: loss = 0.0005578145501203835\n",
      "step = 106000: loss = 0.0016383572947233915\n",
      "step = 106000: Average Return = -200.0\n",
      "step = 106200: loss = 0.004364871419966221\n",
      "step = 106400: loss = 0.0004926167894154787\n",
      "step = 106600: loss = 0.0013759564608335495\n",
      "step = 106800: loss = 0.0028952290304005146\n",
      "step = 107000: loss = 1.0462726354599\n",
      "step = 107000: Average Return = -200.0\n",
      "step = 107200: loss = 0.00387388258241117\n",
      "step = 107400: loss = 0.002927257213741541\n",
      "step = 107600: loss = 0.00248359190300107\n",
      "step = 107800: loss = 0.006276599131524563\n",
      "step = 108000: loss = 0.002383193466812372\n",
      "step = 108000: Average Return = -200.0\n",
      "step = 108200: loss = 0.8600061535835266\n",
      "step = 108400: loss = 1.1741167306900024\n",
      "step = 108600: loss = 0.001348254969343543\n",
      "step = 108800: loss = 0.004141491837799549\n",
      "step = 109000: loss = 0.008308082818984985\n",
      "step = 109000: Average Return = -200.0\n",
      "step = 109200: loss = 0.0016847812803462148\n",
      "step = 109400: loss = 0.9728219509124756\n",
      "step = 109600: loss = 0.002231032820418477\n",
      "step = 109800: loss = 0.0011383482487872243\n",
      "step = 110000: loss = 0.000695833470672369\n",
      "step = 110000: Average Return = -200.0\n",
      "step = 110200: loss = 0.014961067587137222\n",
      "step = 110400: loss = 0.002099752426147461\n",
      "step = 110600: loss = 0.0028900441247969866\n",
      "step = 110800: loss = 0.9588255882263184\n",
      "step = 111000: loss = 1.1460835933685303\n",
      "step = 111000: Average Return = -200.0\n",
      "step = 111200: loss = 1.099943995475769\n",
      "step = 111400: loss = 0.00192583492025733\n",
      "step = 111600: loss = 0.0021764738485217094\n",
      "step = 111800: loss = 0.004293470177799463\n",
      "step = 112000: loss = 0.006660425569862127\n",
      "step = 112000: Average Return = -200.0\n",
      "step = 112200: loss = 0.0009949598461389542\n",
      "step = 112400: loss = 0.00679231621325016\n",
      "step = 112600: loss = 1.081541657447815\n",
      "step = 112800: loss = 0.004174922592937946\n",
      "step = 113000: loss = 0.009899945929646492\n",
      "step = 113000: Average Return = -200.0\n",
      "step = 113200: loss = 1.857497215270996\n",
      "step = 113400: loss = 0.004156121518462896\n",
      "step = 113600: loss = 0.0005618393188342452\n",
      "step = 113800: loss = 0.0035620098933577538\n",
      "step = 114000: loss = 0.0029142098501324654\n",
      "step = 114000: Average Return = -200.0\n",
      "step = 114200: loss = 1.1717054843902588\n",
      "step = 114400: loss = 0.003212182316929102\n",
      "step = 114600: loss = 0.0017687955405563116\n",
      "step = 114800: loss = 0.007605429273098707\n",
      "step = 115000: loss = 0.008787146769464016\n",
      "step = 115000: Average Return = -200.0\n",
      "step = 115200: loss = 2.214726209640503\n",
      "step = 115400: loss = 0.0036997669376432896\n",
      "step = 115600: loss = 0.9763739705085754\n",
      "step = 115800: loss = 0.0015895622782409191\n",
      "step = 116000: loss = 0.005384381860494614\n",
      "step = 116000: Average Return = -200.0\n",
      "step = 116200: loss = 0.0020185208413749933\n",
      "step = 116400: loss = 0.0049362266436219215\n",
      "step = 116600: loss = 2.1107654571533203\n",
      "step = 116800: loss = 0.0007598581723868847\n",
      "step = 117000: loss = 0.004635496065020561\n",
      "step = 117000: Average Return = -200.0\n",
      "step = 117200: loss = 0.0023400033824145794\n",
      "step = 117400: loss = 0.7424532771110535\n",
      "step = 117600: loss = 0.0011851652525365353\n",
      "step = 117800: loss = 0.0057664248161017895\n",
      "step = 118000: loss = 0.004806116223335266\n",
      "step = 118000: Average Return = -200.0\n",
      "step = 118200: loss = 0.0034509762190282345\n",
      "step = 118400: loss = 1.0968778133392334\n",
      "step = 118600: loss = 0.0018148466479033232\n",
      "step = 118800: loss = 0.0020560002885758877\n",
      "step = 119000: loss = 0.0010003676870837808\n",
      "step = 119000: Average Return = -200.0\n",
      "step = 119200: loss = 1.1440136432647705\n",
      "step = 119400: loss = 0.0016324943862855434\n",
      "step = 119600: loss = 0.002922805491834879\n",
      "step = 119800: loss = 0.0011063278652727604\n",
      "step = 120000: loss = 0.0025173272006213665\n",
      "step = 120000: Average Return = -200.0\n",
      "step = 120200: loss = 1.040400505065918\n",
      "step = 120400: loss = 0.0019954959861934185\n",
      "step = 120600: loss = 0.990977942943573\n",
      "step = 120800: loss = 0.0006894476246088743\n",
      "step = 121000: loss = 2.2767441272735596\n",
      "step = 121000: Average Return = -200.0\n",
      "step = 121200: loss = 0.8744199275970459\n",
      "step = 121400: loss = 0.004551870282739401\n",
      "step = 121600: loss = 0.0025881906040012836\n",
      "step = 121800: loss = 0.0047881221398711205\n",
      "step = 122000: loss = 1.0705182552337646\n",
      "step = 122000: Average Return = -200.0\n",
      "step = 122200: loss = 0.00725398026406765\n",
      "step = 122400: loss = 1.1568498611450195\n",
      "step = 122600: loss = 3.183818817138672\n",
      "step = 122800: loss = 0.0016711207572370768\n",
      "step = 123000: loss = 0.008060645312070847\n",
      "step = 123000: Average Return = -200.0\n",
      "step = 123200: loss = 0.005994308739900589\n",
      "step = 123400: loss = 0.006740123964846134\n",
      "step = 123600: loss = 0.0021699173375964165\n",
      "step = 123800: loss = 0.0006985773798078299\n",
      "step = 124000: loss = 0.0012828039471060038\n",
      "step = 124000: Average Return = -200.0\n",
      "step = 124200: loss = 0.0023490949533879757\n",
      "step = 124400: loss = 0.006446615792810917\n",
      "step = 124600: loss = 0.005869024433195591\n",
      "step = 124800: loss = 0.005306384991854429\n",
      "step = 125000: loss = 2.1038737297058105\n",
      "step = 125000: Average Return = -200.0\n",
      "step = 125200: loss = 0.0018524402985349298\n",
      "step = 125400: loss = 2.267801284790039\n",
      "step = 125600: loss = 0.008239764720201492\n",
      "step = 125800: loss = 0.00252695195376873\n",
      "step = 126000: loss = 0.0031340662389993668\n",
      "step = 126000: Average Return = -200.0\n",
      "step = 126200: loss = 0.0042421561665833\n",
      "step = 126400: loss = 0.0022052391432225704\n",
      "step = 126600: loss = 0.011028144508600235\n",
      "step = 126800: loss = 0.008581890724599361\n",
      "step = 127000: loss = 1.0485721826553345\n",
      "step = 127000: Average Return = -200.0\n",
      "step = 127200: loss = 0.007927016355097294\n",
      "step = 127400: loss = 0.005926548969000578\n",
      "step = 127600: loss = 0.00596250407397747\n",
      "step = 127800: loss = 0.0007788474904373288\n",
      "step = 128000: loss = 0.002676859265193343\n",
      "step = 128000: Average Return = -200.0\n",
      "step = 128200: loss = 0.0026925900019705296\n",
      "step = 128400: loss = 0.0017903409898281097\n",
      "step = 128600: loss = 0.0035960376262664795\n",
      "step = 128800: loss = 0.002840243047103286\n",
      "step = 129000: loss = 0.006041604094207287\n",
      "step = 129000: Average Return = -200.0\n",
      "step = 129200: loss = 0.0017001614905893803\n",
      "step = 129400: loss = 0.0019560609944164753\n",
      "step = 129600: loss = 0.0031861700117588043\n",
      "step = 129800: loss = 0.0010702218860387802\n",
      "step = 130000: loss = 0.003342563286423683\n",
      "step = 130000: Average Return = -200.0\n",
      "step = 130200: loss = 0.0015466049080714583\n",
      "step = 130400: loss = 0.0018846220336854458\n",
      "step = 130600: loss = 0.013786261901259422\n",
      "step = 130800: loss = 0.0015437769470736384\n",
      "step = 131000: loss = 1.099524974822998\n",
      "step = 131000: Average Return = -200.0\n",
      "step = 131200: loss = 0.007468346506357193\n",
      "step = 131400: loss = 0.7567651867866516\n",
      "step = 131600: loss = 1.117792010307312\n",
      "step = 131800: loss = 0.012531159445643425\n",
      "step = 132000: loss = 0.002167395083233714\n",
      "step = 132000: Average Return = -200.0\n",
      "step = 132200: loss = 1.1507368087768555\n",
      "step = 132400: loss = 0.004084921441972256\n",
      "step = 132600: loss = 1.1740803718566895\n",
      "step = 132800: loss = 0.0014770356938242912\n",
      "step = 133000: loss = 1.165613055229187\n",
      "step = 133000: Average Return = -200.0\n",
      "step = 133200: loss = 0.003585341852158308\n",
      "step = 133400: loss = 0.003984708338975906\n",
      "step = 133600: loss = 0.001522617181763053\n",
      "step = 133800: loss = 0.0006548887467943132\n",
      "step = 134000: loss = 0.0007994151674211025\n",
      "step = 134000: Average Return = -200.0\n",
      "step = 134200: loss = 0.000800389563664794\n",
      "step = 134400: loss = 1.0892430543899536\n",
      "step = 134600: loss = 0.0007456837338395417\n",
      "step = 134800: loss = 0.005143730901181698\n",
      "step = 135000: loss = 0.0034500963520258665\n",
      "step = 135000: Average Return = -200.0\n",
      "step = 135200: loss = 1.963593602180481\n",
      "step = 135400: loss = 1.1771992444992065\n",
      "step = 135600: loss = 0.001455684076063335\n",
      "step = 135800: loss = 0.0024424938019365072\n",
      "step = 136000: loss = 0.0013845979701727629\n",
      "step = 136000: Average Return = -200.0\n",
      "step = 136200: loss = 0.0009609758271835744\n",
      "step = 136400: loss = 0.0035047619603574276\n",
      "step = 136600: loss = 0.0005644466727972031\n",
      "step = 136800: loss = 1.1781340837478638\n",
      "step = 137000: loss = 0.0014874378684908152\n",
      "step = 137000: Average Return = -200.0\n",
      "step = 137200: loss = 0.0018126878421753645\n",
      "step = 137400: loss = 1.1179920434951782\n",
      "step = 137600: loss = 0.015190022997558117\n",
      "step = 137800: loss = 0.0008923148270696402\n",
      "step = 138000: loss = 0.010813592001795769\n",
      "step = 138000: Average Return = -200.0\n",
      "step = 138200: loss = 1.1629081964492798\n",
      "step = 138400: loss = 0.0009443757589906454\n",
      "step = 138600: loss = 0.0044543067924678326\n",
      "step = 138800: loss = 0.0014410458970814943\n",
      "step = 139000: loss = 0.0032131411135196686\n",
      "step = 139000: Average Return = -200.0\n",
      "step = 139200: loss = 0.0013877276796847582\n",
      "step = 139400: loss = 1.1341123580932617\n",
      "step = 139600: loss = 0.0036728540435433388\n",
      "step = 139800: loss = 0.0010672254720702767\n",
      "step = 140000: loss = 0.010989302769303322\n",
      "step = 140000: Average Return = -200.0\n",
      "step = 140200: loss = 0.0044922493398189545\n",
      "step = 140400: loss = 1.1290240287780762\n",
      "step = 140600: loss = 1.1196495294570923\n",
      "step = 140800: loss = 0.0025862380862236023\n",
      "step = 141000: loss = 0.005397400818765163\n",
      "step = 141000: Average Return = -200.0\n",
      "step = 141200: loss = 0.0016170797171071172\n",
      "step = 141400: loss = 1.1652008295059204\n",
      "step = 141600: loss = 0.0016490498092025518\n",
      "step = 141800: loss = 0.009271116927266121\n",
      "step = 142000: loss = 0.0035214312374591827\n",
      "step = 142000: Average Return = -200.0\n",
      "step = 142200: loss = 0.0016653857892379165\n",
      "step = 142400: loss = 0.0020749373361468315\n",
      "step = 142600: loss = 2.2842469215393066\n",
      "step = 142800: loss = 0.005577774718403816\n",
      "step = 143000: loss = 0.0036442470736801624\n",
      "step = 143000: Average Return = -200.0\n",
      "step = 143200: loss = 0.001377057284116745\n",
      "step = 143400: loss = 0.0020334115251898766\n",
      "step = 143600: loss = 0.0022213002666831017\n",
      "step = 143800: loss = 0.0007984209223650396\n",
      "step = 144000: loss = 0.005146837793290615\n",
      "step = 144000: Average Return = -200.0\n",
      "step = 144200: loss = 0.0011496910592541099\n",
      "step = 144400: loss = 0.007619982119649649\n",
      "step = 144600: loss = 1.1492815017700195\n",
      "step = 144800: loss = 1.1688953638076782\n",
      "step = 145000: loss = 0.0022997246123850346\n",
      "step = 145000: Average Return = -200.0\n",
      "step = 145200: loss = 1.0047509670257568\n",
      "step = 145400: loss = 0.0014511633198708296\n",
      "step = 145600: loss = 2.155118942260742\n",
      "step = 145800: loss = 1.136220932006836\n",
      "step = 146000: loss = 1.165045976638794\n",
      "step = 146000: Average Return = -200.0\n",
      "step = 146200: loss = 0.001413267687894404\n",
      "step = 146400: loss = 1.173223614692688\n",
      "step = 146600: loss = 2.0697438716888428\n",
      "step = 146800: loss = 1.1504287719726562\n",
      "step = 147000: loss = 0.0021537323482334614\n",
      "step = 147000: Average Return = -200.0\n",
      "step = 147200: loss = 0.001502858242020011\n",
      "step = 147400: loss = 0.0023430208675563335\n",
      "step = 147600: loss = 0.0012547108344733715\n",
      "step = 147800: loss = 0.004067191854119301\n",
      "step = 148000: loss = 0.004976126365363598\n",
      "step = 148000: Average Return = -200.0\n",
      "step = 148200: loss = 0.0011335632298141718\n",
      "step = 148400: loss = 0.008658323436975479\n",
      "step = 148600: loss = 0.003452485427260399\n",
      "step = 148800: loss = 1.035969853401184\n",
      "step = 149000: loss = 0.0038771440740674734\n",
      "step = 149000: Average Return = -200.0\n",
      "step = 149200: loss = 1.1342693567276\n",
      "step = 149400: loss = 0.004081900231540203\n",
      "step = 149600: loss = 0.0013391778338700533\n",
      "step = 149800: loss = 0.003639139933511615\n",
      "step = 150000: loss = 1.119415044784546\n",
      "step = 150000: Average Return = -200.0\n",
      "step = 150200: loss = 0.0015160702168941498\n",
      "step = 150400: loss = 0.003884566482156515\n",
      "step = 150600: loss = 0.7830609083175659\n",
      "step = 150800: loss = 0.0018954916158691049\n",
      "step = 151000: loss = 0.8256186842918396\n",
      "step = 151000: Average Return = -200.0\n",
      "step = 151200: loss = 0.0007620005635544658\n",
      "step = 151400: loss = 0.0015257205814123154\n",
      "step = 151600: loss = 0.0018690127180889249\n",
      "step = 151800: loss = 0.00045975911780260503\n",
      "step = 152000: loss = 0.0012246002443134785\n",
      "step = 152000: Average Return = -200.0\n",
      "step = 152200: loss = 1.0405020713806152\n",
      "step = 152400: loss = 0.0036053983494639397\n",
      "step = 152600: loss = 0.001580527750775218\n",
      "step = 152800: loss = 0.01571328192949295\n",
      "step = 153000: loss = 3.3831706047058105\n",
      "step = 153000: Average Return = -200.0\n",
      "step = 153200: loss = 1.1460152864456177\n",
      "step = 153400: loss = 1.106554627418518\n",
      "step = 153600: loss = 0.004299275577068329\n",
      "step = 153800: loss = 0.012959171086549759\n",
      "step = 154000: loss = 0.007220910862088203\n",
      "step = 154000: Average Return = -200.0\n",
      "step = 154200: loss = 0.8552721738815308\n",
      "step = 154400: loss = 0.004102632403373718\n",
      "step = 154600: loss = 0.0010548860300332308\n",
      "step = 154800: loss = 0.0012549892999231815\n",
      "step = 155000: loss = 0.005110599100589752\n",
      "step = 155000: Average Return = -200.0\n",
      "step = 155200: loss = 1.1623094081878662\n",
      "step = 155400: loss = 1.1805357933044434\n",
      "step = 155600: loss = 0.0026309285312891006\n",
      "step = 155800: loss = 0.0035988404415547848\n",
      "step = 156000: loss = 0.00212527671828866\n",
      "step = 156000: Average Return = -200.0\n",
      "step = 156200: loss = 0.0028588157147169113\n",
      "step = 156400: loss = 0.002868611365556717\n",
      "step = 156600: loss = 1.1261086463928223\n",
      "step = 156800: loss = 0.0015488732606172562\n",
      "step = 157000: loss = 1.1778361797332764\n",
      "step = 157000: Average Return = -200.0\n",
      "step = 157200: loss = 0.008659765124320984\n",
      "step = 157400: loss = 1.162007451057434\n",
      "step = 157600: loss = 0.0020594638772308826\n",
      "step = 157800: loss = 0.0018597814487293363\n",
      "step = 158000: loss = 0.00517707783728838\n",
      "step = 158000: Average Return = -200.0\n",
      "step = 158200: loss = 1.0536165237426758\n",
      "step = 158400: loss = 0.0007744012400507927\n",
      "step = 158600: loss = 0.0009520978201180696\n",
      "step = 158800: loss = 0.003938549198210239\n",
      "step = 159000: loss = 1.0847654342651367\n",
      "step = 159000: Average Return = -200.0\n",
      "step = 159200: loss = 0.002760959090664983\n",
      "step = 159400: loss = 0.0018059590365737677\n",
      "step = 159600: loss = 0.005523309577256441\n",
      "step = 159800: loss = 0.006134827621281147\n",
      "step = 160000: loss = 0.0036051475908607244\n",
      "step = 160000: Average Return = -200.0\n",
      "step = 160200: loss = 0.0012899909634143114\n",
      "step = 160400: loss = 0.002690376015380025\n",
      "step = 160600: loss = 0.0016970641445368528\n",
      "step = 160800: loss = 0.007716835010796785\n",
      "step = 161000: loss = 0.004612761549651623\n",
      "step = 161000: Average Return = -200.0\n",
      "step = 161200: loss = 1.128732442855835\n",
      "step = 161400: loss = 0.00138171820435673\n",
      "step = 161600: loss = 0.007135357242077589\n",
      "step = 161800: loss = 0.0039522647857666016\n",
      "step = 162000: loss = 0.0012318738736212254\n",
      "step = 162000: Average Return = -200.0\n",
      "step = 162200: loss = 0.003143243957310915\n",
      "step = 162400: loss = 0.004282176028937101\n",
      "step = 162600: loss = 0.0030720322392880917\n",
      "step = 162800: loss = 0.0024572848342359066\n",
      "step = 163000: loss = 1.1039705276489258\n",
      "step = 163000: Average Return = -200.0\n",
      "step = 163200: loss = 1.1208387613296509\n",
      "step = 163400: loss = 0.007309622131288052\n",
      "step = 163600: loss = 1.1055240631103516\n",
      "step = 163800: loss = 0.0033570786472409964\n",
      "step = 164000: loss = 0.005333886481821537\n",
      "step = 164000: Average Return = -200.0\n",
      "step = 164200: loss = 0.005639408249408007\n",
      "step = 164400: loss = 1.858640193939209\n",
      "step = 164600: loss = 0.0014877398498356342\n",
      "step = 164800: loss = 1.1246381998062134\n",
      "step = 165000: loss = 1.1505436897277832\n",
      "step = 165000: Average Return = -200.0\n",
      "step = 165200: loss = 0.0012145540677011013\n",
      "step = 165400: loss = 1.1878465414047241\n",
      "step = 165600: loss = 0.0015497657004743814\n",
      "step = 165800: loss = 1.1399413347244263\n",
      "step = 166000: loss = 1.172947645187378\n",
      "step = 166000: Average Return = -200.0\n",
      "step = 166200: loss = 0.0025592255406081676\n",
      "step = 166400: loss = 0.9654670357704163\n",
      "step = 166600: loss = 1.0325266122817993\n",
      "step = 166800: loss = 1.0808459520339966\n",
      "step = 167000: loss = 0.0022210481110960245\n",
      "step = 167000: Average Return = -200.0\n",
      "step = 167200: loss = 0.005369857884943485\n",
      "step = 167400: loss = 0.7940120100975037\n",
      "step = 167600: loss = 3.3264012336730957\n",
      "step = 167800: loss = 1.0396705865859985\n",
      "step = 168000: loss = 0.9740562438964844\n",
      "step = 168000: Average Return = -200.0\n",
      "step = 168200: loss = 0.0057322438806295395\n",
      "step = 168400: loss = 0.000891704810783267\n",
      "step = 168600: loss = 0.0017915284261107445\n",
      "step = 168800: loss = 0.001873846398666501\n",
      "step = 169000: loss = 1.1290792226791382\n",
      "step = 169000: Average Return = -200.0\n",
      "step = 169200: loss = 0.003761442843824625\n",
      "step = 169400: loss = 1.0797264575958252\n",
      "step = 169600: loss = 1.0844794511795044\n",
      "step = 169800: loss = 0.001299134106375277\n",
      "step = 170000: loss = 0.0012739034136757255\n",
      "step = 170000: Average Return = -200.0\n",
      "step = 170200: loss = 0.004432129207998514\n",
      "step = 170400: loss = 0.001024914556182921\n",
      "step = 170600: loss = 1.1844196319580078\n",
      "step = 170800: loss = 0.0017693903064355254\n",
      "step = 171000: loss = 0.0048029255121946335\n",
      "step = 171000: Average Return = -200.0\n",
      "step = 171200: loss = 0.0038668906781822443\n",
      "step = 171400: loss = 0.006039406172931194\n",
      "step = 171600: loss = 0.002085904125124216\n",
      "step = 171800: loss = 0.004978219047188759\n",
      "step = 172000: loss = 0.002986183390021324\n",
      "step = 172000: Average Return = -200.0\n",
      "step = 172200: loss = 0.0010375166311860085\n",
      "step = 172400: loss = 0.002910867566242814\n",
      "step = 172600: loss = 0.00468306802213192\n",
      "step = 172800: loss = 0.004874683450907469\n",
      "step = 173000: loss = 0.7666726112365723\n",
      "step = 173000: Average Return = -200.0\n",
      "step = 173200: loss = 0.005578638985753059\n",
      "step = 173400: loss = 0.011808552779257298\n",
      "step = 173600: loss = 1.0253366231918335\n",
      "step = 173800: loss = 0.006027946248650551\n",
      "step = 174000: loss = 0.0013777847634628415\n",
      "step = 174000: Average Return = -200.0\n",
      "step = 174200: loss = 0.002710069762542844\n",
      "step = 174400: loss = 0.006637269631028175\n",
      "step = 174600: loss = 1.1136776208877563\n",
      "step = 174800: loss = 0.003105146810412407\n",
      "step = 175000: loss = 0.010462827049195766\n",
      "step = 175000: Average Return = -200.0\n",
      "step = 175200: loss = 0.0025740121491253376\n",
      "step = 175400: loss = 1.1189665794372559\n",
      "step = 175600: loss = 0.004452382680028677\n",
      "step = 175800: loss = 0.007288905791938305\n",
      "step = 176000: loss = 0.002964212093502283\n",
      "step = 176000: Average Return = -200.0\n",
      "step = 176200: loss = 0.0046608541160821915\n",
      "step = 176400: loss = 0.0031807227060198784\n",
      "step = 176600: loss = 0.008262497372925282\n",
      "step = 176800: loss = 0.010010894387960434\n",
      "step = 177000: loss = 0.0010958509519696236\n",
      "step = 177000: Average Return = -200.0\n",
      "step = 177200: loss = 0.008656194433569908\n",
      "step = 177400: loss = 0.004233280196785927\n",
      "step = 177600: loss = 0.009036105126142502\n",
      "step = 177800: loss = 2.0162787437438965\n",
      "step = 178000: loss = 1.172234058380127\n",
      "step = 178000: Average Return = -200.0\n",
      "step = 178200: loss = 1.117990493774414\n",
      "step = 178400: loss = 0.005005033686757088\n",
      "step = 178600: loss = 1.8820171356201172\n",
      "step = 178800: loss = 1.0208097696304321\n",
      "step = 179000: loss = 2.1068031787872314\n",
      "step = 179000: Average Return = -200.0\n",
      "step = 179200: loss = 0.0029513416811823845\n",
      "step = 179400: loss = 0.006026850081980228\n",
      "step = 179600: loss = 1.0787417888641357\n",
      "step = 179800: loss = 1.1349905729293823\n",
      "step = 180000: loss = 0.012750710360705853\n",
      "step = 180000: Average Return = -200.0\n",
      "step = 180200: loss = 0.000989822088740766\n",
      "step = 180400: loss = 0.0061815897934138775\n",
      "step = 180600: loss = 0.006514270789921284\n",
      "step = 180800: loss = 0.012714032083749771\n",
      "step = 181000: loss = 1.929419755935669\n",
      "step = 181000: Average Return = -200.0\n",
      "step = 181200: loss = 0.0028933139983564615\n",
      "step = 181400: loss = 0.007790979463607073\n",
      "step = 181600: loss = 0.010639798827469349\n",
      "step = 181800: loss = 2.1983566284179688\n",
      "step = 182000: loss = 2.2715539932250977\n",
      "step = 182000: Average Return = -200.0\n",
      "step = 182200: loss = 0.007823844440281391\n",
      "step = 182400: loss = 0.002476213965564966\n",
      "step = 182600: loss = 0.0015624795341864228\n",
      "step = 182800: loss = 0.6598830819129944\n",
      "step = 183000: loss = 0.00227601220831275\n",
      "step = 183000: Average Return = -200.0\n",
      "step = 183200: loss = 1.0703999996185303\n",
      "step = 183400: loss = 0.0016962175723165274\n",
      "step = 183600: loss = 0.0010533577296882868\n",
      "step = 183800: loss = 1.1584668159484863\n",
      "step = 184000: loss = 2.2523207664489746\n",
      "step = 184000: Average Return = -200.0\n",
      "step = 184200: loss = 0.0037007771898061037\n",
      "step = 184400: loss = 0.6240147352218628\n",
      "step = 184600: loss = 1.138602375984192\n",
      "step = 184800: loss = 0.5951673984527588\n",
      "step = 185000: loss = 0.00343992467969656\n",
      "step = 185000: Average Return = -200.0\n",
      "step = 185200: loss = 0.0008806388359516859\n",
      "step = 185400: loss = 0.005616815760731697\n",
      "step = 185600: loss = 0.0025083418004214764\n",
      "step = 185800: loss = 1.131676435470581\n",
      "step = 186000: loss = 1.1424009799957275\n",
      "step = 186000: Average Return = -200.0\n",
      "step = 186200: loss = 0.001674295519478619\n",
      "step = 186400: loss = 0.007108618970960379\n",
      "step = 186600: loss = 0.002154248533770442\n",
      "step = 186800: loss = 0.0029296018183231354\n",
      "step = 187000: loss = 0.0012320682872086763\n",
      "step = 187000: Average Return = -200.0\n",
      "step = 187200: loss = 0.0021401378326117992\n",
      "step = 187400: loss = 0.0011032791808247566\n",
      "step = 187600: loss = 0.0113206272944808\n",
      "step = 187800: loss = 0.004508012440055609\n",
      "step = 188000: loss = 0.0025242320261895657\n",
      "step = 188000: Average Return = -200.0\n",
      "step = 188200: loss = 1.1874334812164307\n",
      "step = 188400: loss = 0.006726742256432772\n",
      "step = 188600: loss = 0.8928170204162598\n",
      "step = 188800: loss = 0.0023441335652023554\n",
      "step = 189000: loss = 0.0011274800635874271\n",
      "step = 189000: Average Return = -200.0\n",
      "step = 189200: loss = 0.002314203418791294\n",
      "step = 189400: loss = 0.0006239715148694813\n",
      "step = 189600: loss = 0.0020540899131447077\n",
      "step = 189800: loss = 0.006317680701613426\n",
      "step = 190000: loss = 0.009140724316239357\n",
      "step = 190000: Average Return = -200.0\n",
      "step = 190200: loss = 0.004249952733516693\n",
      "step = 190400: loss = 1.0721396207809448\n",
      "step = 190600: loss = 0.0017989989137277007\n",
      "step = 190800: loss = 0.0024486719630658627\n",
      "step = 191000: loss = 0.0053971437737345695\n",
      "step = 191000: Average Return = -200.0\n",
      "step = 191200: loss = 0.0018561063334345818\n",
      "step = 191400: loss = 1.7917808294296265\n",
      "step = 191600: loss = 0.003963838797062635\n",
      "step = 191800: loss = 0.001709087286144495\n",
      "step = 192000: loss = 0.0008150136563926935\n",
      "step = 192000: Average Return = -200.0\n",
      "step = 192200: loss = 0.005084877833724022\n",
      "step = 192400: loss = 1.166029930114746\n",
      "step = 192600: loss = 0.010154787451028824\n",
      "step = 192800: loss = 0.004061075858771801\n",
      "step = 193000: loss = 0.003481469349935651\n",
      "step = 193000: Average Return = -200.0\n",
      "step = 193200: loss = 0.002085990272462368\n",
      "step = 193400: loss = 0.0030132890678942204\n",
      "step = 193600: loss = 2.273204803466797\n",
      "step = 193800: loss = 2.2770400047302246\n",
      "step = 194000: loss = 1.1025320291519165\n",
      "step = 194000: Average Return = -200.0\n",
      "step = 194200: loss = 1.092265009880066\n",
      "step = 194400: loss = 2.0603559017181396\n",
      "step = 194600: loss = 0.0014790624845772982\n",
      "step = 194800: loss = 0.0020981808193027973\n",
      "step = 195000: loss = 2.0108742713928223\n",
      "step = 195000: Average Return = -200.0\n",
      "step = 195200: loss = 0.0027239136397838593\n",
      "step = 195400: loss = 1.1903325319290161\n",
      "step = 195600: loss = 0.002983079059049487\n",
      "step = 195800: loss = 0.0024810871109366417\n",
      "step = 196000: loss = 0.008089039474725723\n",
      "step = 196000: Average Return = -200.0\n",
      "step = 196200: loss = 0.003825882915407419\n",
      "step = 196400: loss = 0.01141554955393076\n",
      "step = 196600: loss = 0.003251009387895465\n",
      "step = 196800: loss = 1.8951283693313599\n",
      "step = 197000: loss = 0.0021265349350869656\n",
      "step = 197000: Average Return = -200.0\n",
      "step = 197200: loss = 0.0013027049135416746\n",
      "step = 197400: loss = 0.001656150445342064\n",
      "step = 197600: loss = 0.0008928432944230735\n",
      "step = 197800: loss = 0.9282911419868469\n",
      "step = 198000: loss = 0.002296015853062272\n",
      "step = 198000: Average Return = -200.0\n",
      "step = 198200: loss = 0.0038304715417325497\n",
      "step = 198400: loss = 0.0029294374398887157\n",
      "step = 198600: loss = 1.0487602949142456\n",
      "step = 198800: loss = 1.0831098556518555\n",
      "step = 199000: loss = 0.0017443066462874413\n",
      "step = 199000: Average Return = -200.0\n",
      "step = 199200: loss = 1.1931533813476562\n",
      "step = 199400: loss = 1.0365421772003174\n",
      "step = 199600: loss = 0.004530024249106646\n",
      "step = 199800: loss = 0.0008959349943324924\n",
      "step = 200000: loss = 1.0758720636367798\n",
      "step = 200000: Average Return = -200.0\n",
      "step = 200200: loss = 0.0013471178244799376\n",
      "step = 200400: loss = 0.0026844877284020185\n",
      "step = 200600: loss = 0.005706534720957279\n",
      "step = 200800: loss = 0.006039556581526995\n",
      "step = 201000: loss = 0.02110915258526802\n",
      "step = 201000: Average Return = -200.0\n",
      "step = 201200: loss = 1.1573716402053833\n",
      "step = 201400: loss = 1.1703869104385376\n",
      "step = 201600: loss = 1.1753185987472534\n",
      "step = 201800: loss = 0.0017386702820658684\n",
      "step = 202000: loss = 1.1168502569198608\n",
      "step = 202000: Average Return = -200.0\n",
      "step = 202200: loss = 0.005505112931132317\n",
      "step = 202400: loss = 0.0007808689842931926\n",
      "step = 202600: loss = 0.6943531632423401\n",
      "step = 202800: loss = 0.00267904344946146\n",
      "step = 203000: loss = 0.0036106687039136887\n",
      "step = 203000: Average Return = -200.0\n",
      "step = 203200: loss = 1.0516124963760376\n",
      "step = 203400: loss = 0.004413013346493244\n",
      "step = 203600: loss = 0.008195353671908379\n",
      "step = 203800: loss = 0.001664275536313653\n",
      "step = 204000: loss = 0.002674175426363945\n",
      "step = 204000: Average Return = -200.0\n",
      "step = 204200: loss = 0.0011142604053020477\n",
      "step = 204400: loss = 0.012430990114808083\n",
      "step = 204600: loss = 0.0038288035430014133\n",
      "step = 204800: loss = 0.0029528033919632435\n",
      "step = 205000: loss = 2.1683688163757324\n",
      "step = 205000: Average Return = -200.0\n",
      "step = 205200: loss = 2.7149527072906494\n",
      "step = 205400: loss = 0.001812207163311541\n",
      "step = 205600: loss = 0.009614340960979462\n",
      "step = 205800: loss = 0.002305231522768736\n",
      "step = 206000: loss = 0.0038109805900603533\n",
      "step = 206000: Average Return = -200.0\n",
      "step = 206200: loss = 0.002529669785872102\n",
      "step = 206400: loss = 0.010420344769954681\n",
      "step = 206600: loss = 0.0038781315088272095\n",
      "step = 206800: loss = 0.0021278841886669397\n",
      "step = 207000: loss = 0.001206171466037631\n",
      "step = 207000: Average Return = -200.0\n",
      "step = 207200: loss = 0.005428559146821499\n",
      "step = 207400: loss = 0.002978942357003689\n",
      "step = 207600: loss = 0.008712857030332088\n",
      "step = 207800: loss = 0.0018405582522973418\n",
      "step = 208000: loss = 0.0034710189793258905\n",
      "step = 208000: Average Return = -200.0\n",
      "step = 208200: loss = 0.002261010929942131\n",
      "step = 208400: loss = 0.0037143544759601355\n",
      "step = 208600: loss = 0.0014930317411199212\n",
      "step = 208800: loss = 1.1553452014923096\n",
      "step = 209000: loss = 0.0041383919306099415\n",
      "step = 209000: Average Return = -200.0\n",
      "step = 209200: loss = 0.0036523512098938227\n",
      "step = 209400: loss = 0.006021822802722454\n",
      "step = 209600: loss = 1.1837129592895508\n",
      "step = 209800: loss = 0.004969196394085884\n",
      "step = 210000: loss = 0.0026142389979213476\n",
      "step = 210000: Average Return = -200.0\n",
      "step = 210200: loss = 0.0049813287332654\n",
      "step = 210400: loss = 1.0923678874969482\n",
      "step = 210600: loss = 0.0034170695580542088\n",
      "step = 210800: loss = 1.0141782760620117\n",
      "step = 211000: loss = 0.0014534405199810863\n",
      "step = 211000: Average Return = -200.0\n",
      "step = 211200: loss = 0.004044706467539072\n",
      "step = 211400: loss = 0.005756882019340992\n",
      "step = 211600: loss = 0.002397720469161868\n",
      "step = 211800: loss = 0.002018850529566407\n",
      "step = 212000: loss = 0.0009610042907297611\n",
      "step = 212000: Average Return = -200.0\n",
      "step = 212200: loss = 0.003288986161351204\n",
      "step = 212400: loss = 0.0015096693532541394\n",
      "step = 212600: loss = 0.0026803244836628437\n",
      "step = 212800: loss = 0.001083218608982861\n",
      "step = 213000: loss = 1.1754215955734253\n",
      "step = 213000: Average Return = -200.0\n",
      "step = 213200: loss = 0.9074782133102417\n",
      "step = 213400: loss = 0.007193826138973236\n",
      "step = 213600: loss = 0.002877760911360383\n",
      "step = 213800: loss = 0.004844821058213711\n",
      "step = 214000: loss = 1.0108281373977661\n",
      "step = 214000: Average Return = -200.0\n",
      "step = 214200: loss = 0.004353650845587254\n",
      "step = 214400: loss = 0.004660340026021004\n",
      "step = 214600: loss = 0.003285480896010995\n",
      "step = 214800: loss = 0.0021980307064950466\n",
      "step = 215000: loss = 0.004394397139549255\n",
      "step = 215000: Average Return = -200.0\n",
      "step = 215200: loss = 0.006062800996005535\n",
      "step = 215400: loss = 0.004963201470673084\n",
      "step = 215600: loss = 0.0014797757612541318\n",
      "step = 215800: loss = 1.11204993724823\n",
      "step = 216000: loss = 0.0035260405857115984\n",
      "step = 216000: Average Return = -200.0\n",
      "step = 216200: loss = 0.0047159153036773205\n",
      "step = 216400: loss = 0.008395908400416374\n",
      "step = 216600: loss = 0.004490844439715147\n",
      "step = 216800: loss = 0.0035970353055745363\n",
      "step = 217000: loss = 0.00458676740527153\n",
      "step = 217000: Average Return = -200.0\n",
      "step = 217200: loss = 0.0025512634310871363\n",
      "step = 217400: loss = 0.8690312504768372\n",
      "step = 217600: loss = 0.007604964543133974\n",
      "step = 217800: loss = 0.011254167184233665\n",
      "step = 218000: loss = 0.0016795299015939236\n",
      "step = 218000: Average Return = -200.0\n",
      "step = 218200: loss = 0.0032989601604640484\n",
      "step = 218400: loss = 0.015128020197153091\n",
      "step = 218600: loss = 0.6568673849105835\n",
      "step = 218800: loss = 0.009701606817543507\n",
      "step = 219000: loss = 0.0026294048875570297\n",
      "step = 219000: Average Return = -200.0\n",
      "step = 219200: loss = 0.00894017331302166\n",
      "step = 219400: loss = 0.0030674266163259745\n",
      "step = 219600: loss = 0.010283864103257656\n",
      "step = 219800: loss = 1.0857326984405518\n",
      "step = 220000: loss = 0.002546244766563177\n",
      "step = 220000: Average Return = -200.0\n",
      "step = 220200: loss = 1.0303494930267334\n",
      "step = 220400: loss = 0.006122061982750893\n",
      "step = 220600: loss = 0.002077379496768117\n",
      "step = 220800: loss = 0.0019778842106461525\n",
      "step = 221000: loss = 0.0009734879713505507\n",
      "step = 221000: Average Return = -200.0\n",
      "step = 221200: loss = 0.0012047560885548592\n",
      "step = 221400: loss = 0.005717321764677763\n",
      "step = 221600: loss = 0.8146875500679016\n",
      "step = 221800: loss = 0.7376397252082825\n",
      "step = 222000: loss = 2.1260628700256348\n",
      "step = 222000: Average Return = -200.0\n",
      "step = 222200: loss = 0.0009702406241558492\n",
      "step = 222400: loss = 1.2128511667251587\n",
      "step = 222600: loss = 0.002211406361311674\n",
      "step = 222800: loss = 0.8976851105690002\n",
      "step = 223000: loss = 0.008074125275015831\n",
      "step = 223000: Average Return = -200.0\n",
      "step = 223200: loss = 0.9225316047668457\n",
      "step = 223400: loss = 0.0030454399529844522\n",
      "step = 223600: loss = 0.0014219831209629774\n",
      "step = 223800: loss = 0.002574475947767496\n",
      "step = 224000: loss = 0.0014028018340468407\n",
      "step = 224000: Average Return = -200.0\n",
      "step = 224200: loss = 2.3060081005096436\n",
      "step = 224400: loss = 0.007687910459935665\n",
      "step = 224600: loss = 0.8538873791694641\n",
      "step = 224800: loss = 1.1280245780944824\n",
      "step = 225000: loss = 0.0013358112191781402\n",
      "step = 225000: Average Return = -200.0\n",
      "step = 225200: loss = 0.9642186760902405\n",
      "step = 225400: loss = 0.001539595308713615\n",
      "step = 225600: loss = 0.005910303443670273\n",
      "step = 225800: loss = 0.0012537689181044698\n",
      "step = 226000: loss = 0.002638152102008462\n",
      "step = 226000: Average Return = -200.0\n",
      "step = 226200: loss = 0.004497168585658073\n",
      "step = 226400: loss = 0.022220641374588013\n",
      "step = 226600: loss = 0.0013223430141806602\n",
      "step = 226800: loss = 0.007909711450338364\n",
      "step = 227000: loss = 1.0810801982879639\n",
      "step = 227000: Average Return = -200.0\n",
      "step = 227200: loss = 0.0019326935289427638\n",
      "step = 227400: loss = 0.0012582726776599884\n",
      "step = 227600: loss = 0.002921822015196085\n",
      "step = 227800: loss = 0.002648988738656044\n",
      "step = 228000: loss = 0.006460950709879398\n",
      "step = 228000: Average Return = -200.0\n",
      "step = 228200: loss = 0.0027173839043825865\n",
      "step = 228400: loss = 0.9947503805160522\n",
      "step = 228600: loss = 0.005389223340898752\n",
      "step = 228800: loss = 0.001880172872915864\n",
      "step = 229000: loss = 2.25822114944458\n",
      "step = 229000: Average Return = -200.0\n",
      "step = 229200: loss = 0.00317072169855237\n",
      "step = 229400: loss = 0.001019860035739839\n",
      "step = 229600: loss = 1.1688498258590698\n",
      "step = 229800: loss = 0.0020152139477431774\n",
      "step = 230000: loss = 0.0054946113377809525\n",
      "step = 230000: Average Return = -200.0\n",
      "step = 230200: loss = 1.082655668258667\n",
      "step = 230400: loss = 0.0018321857787668705\n",
      "step = 230600: loss = 0.0010970481671392918\n",
      "step = 230800: loss = 0.002726325998082757\n",
      "step = 231000: loss = 1.1266719102859497\n",
      "step = 231000: Average Return = -141.5\n",
      "step = 231200: loss = 0.0011032361071556807\n",
      "step = 231400: loss = 0.0011402610689401627\n",
      "step = 231600: loss = 1.174912691116333\n",
      "step = 231800: loss = 0.00224634213373065\n",
      "step = 232000: loss = 3.377397060394287\n",
      "step = 232000: Average Return = -158.1999969482422\n",
      "step = 232200: loss = 0.005213682074099779\n",
      "step = 232400: loss = 0.0018244943348690867\n",
      "step = 232600: loss = 1.0698364973068237\n",
      "step = 232800: loss = 2.2651541233062744\n",
      "step = 233000: loss = 1.1715264320373535\n",
      "step = 233000: Average Return = -200.0\n",
      "step = 233200: loss = 0.015146937221288681\n",
      "step = 233400: loss = 0.001974610611796379\n",
      "step = 233600: loss = 0.005341686308383942\n",
      "step = 233800: loss = 0.0015649317065253854\n",
      "step = 234000: loss = 0.005908193066716194\n",
      "step = 234000: Average Return = -198.89999389648438\n",
      "step = 234200: loss = 1.8490190505981445\n",
      "step = 234400: loss = 0.001979729626327753\n",
      "step = 234600: loss = 0.0029841624200344086\n",
      "step = 234800: loss = 1.0156188011169434\n",
      "step = 235000: loss = 0.0034700934775173664\n",
      "step = 235000: Average Return = -200.0\n",
      "step = 235200: loss = 1.0324903726577759\n",
      "step = 235400: loss = 0.004426504019647837\n",
      "step = 235600: loss = 0.0024966918863356113\n",
      "step = 235800: loss = 0.0036162021569907665\n",
      "step = 236000: loss = 0.007251132745295763\n",
      "step = 236000: Average Return = -200.0\n",
      "step = 236200: loss = 0.002508860547095537\n",
      "step = 236400: loss = 1.0176023244857788\n",
      "step = 236600: loss = 0.002553899772465229\n",
      "step = 236800: loss = 0.003826045198366046\n",
      "step = 237000: loss = 0.003921430557966232\n",
      "step = 237000: Average Return = -138.1999969482422\n",
      "step = 237200: loss = 0.005459087900817394\n",
      "step = 237400: loss = 0.0040701935067772865\n",
      "step = 237600: loss = 0.002332170493900776\n",
      "step = 237800: loss = 0.003813869319856167\n",
      "step = 238000: loss = 0.0025835465639829636\n",
      "step = 238000: Average Return = -176.39999389648438\n",
      "step = 238200: loss = 2.2682197093963623\n",
      "step = 238400: loss = 0.0013235537335276604\n",
      "step = 238600: loss = 0.0040352363139390945\n",
      "step = 238800: loss = 0.004820363596081734\n",
      "step = 239000: loss = 1.1472293138504028\n",
      "step = 239000: Average Return = -200.0\n",
      "step = 239200: loss = 2.0477585792541504\n",
      "step = 239400: loss = 0.0036592641845345497\n",
      "step = 239600: loss = 0.7160459160804749\n",
      "step = 239800: loss = 0.0029926467686891556\n",
      "step = 240000: loss = 0.004965633619576693\n",
      "step = 240000: Average Return = -148.1999969482422\n",
      "step = 240200: loss = 0.0019146568374708295\n",
      "step = 240400: loss = 0.0012430488131940365\n",
      "step = 240600: loss = 1.1926617622375488\n",
      "step = 240800: loss = 0.004634014796465635\n",
      "step = 241000: loss = 0.011761205270886421\n",
      "step = 241000: Average Return = -200.0\n",
      "step = 241200: loss = 1.0945632457733154\n",
      "step = 241400: loss = 0.002201666356995702\n",
      "step = 241600: loss = 0.0009641487849876285\n",
      "step = 241800: loss = 0.0013779252767562866\n",
      "step = 242000: loss = 1.172510027885437\n",
      "step = 242000: Average Return = -176.89999389648438\n",
      "step = 242200: loss = 0.0013427369995042682\n",
      "step = 242400: loss = 0.001084919087588787\n",
      "step = 242600: loss = 0.0021860883571207523\n",
      "step = 242800: loss = 0.0034775743260979652\n",
      "step = 243000: loss = 1.1093370914459229\n",
      "step = 243000: Average Return = -140.1999969482422\n",
      "step = 243200: loss = 0.833358108997345\n",
      "step = 243400: loss = 0.0016496594762429595\n",
      "step = 243600: loss = 0.0044579640962183475\n",
      "step = 243800: loss = 0.0018795640207827091\n",
      "step = 244000: loss = 1.0087889432907104\n",
      "step = 244000: Average Return = -200.0\n",
      "step = 244200: loss = 0.0015700592193752527\n",
      "step = 244400: loss = 0.0019485664088279009\n",
      "step = 244600: loss = 0.0029479642398655415\n",
      "step = 244800: loss = 0.0005183872417546809\n",
      "step = 245000: loss = 1.133791446685791\n",
      "step = 245000: Average Return = -133.3000030517578\n",
      "step = 245200: loss = 0.001956602558493614\n",
      "step = 245400: loss = 0.003565188031643629\n",
      "step = 245600: loss = 1.1184619665145874\n",
      "step = 245800: loss = 0.00185483880341053\n",
      "step = 246000: loss = 0.004275616258382797\n",
      "step = 246000: Average Return = -200.0\n",
      "step = 246200: loss = 0.6228073835372925\n",
      "step = 246400: loss = 0.0025981743820011616\n",
      "step = 246600: loss = 0.008872859179973602\n",
      "step = 246800: loss = 0.00275215320289135\n",
      "step = 247000: loss = 0.0024489769712090492\n",
      "step = 247000: Average Return = -194.8000030517578\n",
      "step = 247200: loss = 1.1960307359695435\n",
      "step = 247400: loss = 2.2367844581604004\n",
      "step = 247600: loss = 0.004952973686158657\n",
      "step = 247800: loss = 0.0048255096189677715\n",
      "step = 248000: loss = 0.01617259532213211\n",
      "step = 248000: Average Return = -200.0\n",
      "step = 248200: loss = 0.004131729248911142\n",
      "step = 248400: loss = 0.002916568424552679\n",
      "step = 248600: loss = 0.0018273273017257452\n",
      "step = 248800: loss = 0.0010616871295496821\n",
      "step = 249000: loss = 0.004669717978686094\n",
      "step = 249000: Average Return = -156.6999969482422\n",
      "step = 249200: loss = 0.002681858604773879\n",
      "step = 249400: loss = 1.2102642059326172\n",
      "step = 249600: loss = 0.0013726522447541356\n",
      "step = 249800: loss = 0.0028834366239607334\n",
      "step = 250000: loss = 0.003584618680179119\n",
      "step = 250000: Average Return = -193.0\n",
      "step = 250200: loss = 0.002259251894429326\n",
      "step = 250400: loss = 0.0014685370260849595\n",
      "step = 250600: loss = 0.3879016935825348\n",
      "step = 250800: loss = 0.003420126624405384\n",
      "step = 251000: loss = 1.1598938703536987\n",
      "step = 251000: Average Return = -200.0\n",
      "step = 251200: loss = 0.0026917094364762306\n",
      "step = 251400: loss = 0.9919414520263672\n",
      "step = 251600: loss = 0.0012131332186982036\n",
      "step = 251800: loss = 1.0940825939178467\n",
      "step = 252000: loss = 0.0016749026253819466\n",
      "step = 252000: Average Return = -168.89999389648438\n",
      "step = 252200: loss = 0.9471933841705322\n",
      "step = 252400: loss = 0.004316072911024094\n",
      "step = 252600: loss = 0.004754567518830299\n",
      "step = 252800: loss = 0.0012710144510492682\n",
      "step = 253000: loss = 0.0037391469813883305\n",
      "step = 253000: Average Return = -140.89999389648438\n",
      "step = 253200: loss = 0.0020268699154257774\n",
      "step = 253400: loss = 0.005566829815506935\n",
      "step = 253600: loss = 1.152699589729309\n",
      "step = 253800: loss = 1.1428561210632324\n",
      "step = 254000: loss = 0.0025477558374404907\n",
      "step = 254000: Average Return = -200.0\n",
      "step = 254200: loss = 0.004907655529677868\n",
      "step = 254400: loss = 0.003357301466166973\n",
      "step = 254600: loss = 0.003783776890486479\n",
      "step = 254800: loss = 0.006410178262740374\n",
      "step = 255000: loss = 0.0030821054242551327\n",
      "step = 255000: Average Return = -175.39999389648438\n",
      "step = 255200: loss = 1.1152399778366089\n",
      "step = 255400: loss = 0.005939406342804432\n",
      "step = 255600: loss = 0.0014435576740652323\n",
      "step = 255800: loss = 0.003211475443094969\n",
      "step = 256000: loss = 0.0038175375666469336\n",
      "step = 256000: Average Return = -191.89999389648438\n",
      "step = 256200: loss = 1.094300389289856\n",
      "step = 256400: loss = 0.002186969155445695\n",
      "step = 256600: loss = 0.017147252336144447\n",
      "step = 256800: loss = 0.9356609582901001\n",
      "step = 257000: loss = 0.003293535439297557\n",
      "step = 257000: Average Return = -176.39999389648438\n",
      "step = 257200: loss = 1.0104860067367554\n",
      "step = 257400: loss = 0.0010503027588129044\n",
      "step = 257600: loss = 0.004361795261502266\n",
      "step = 257800: loss = 0.0020057607907801867\n",
      "step = 258000: loss = 0.003788150381296873\n",
      "step = 258000: Average Return = -198.89999389648438\n",
      "step = 258200: loss = 0.003266253974288702\n",
      "step = 258400: loss = 0.0012197609758004546\n",
      "step = 258600: loss = 1.099061131477356\n",
      "step = 258800: loss = 0.0025319012347608805\n",
      "step = 259000: loss = 0.0036694230511784554\n",
      "step = 259000: Average Return = -200.0\n",
      "step = 259200: loss = 0.7461507320404053\n",
      "step = 259400: loss = 0.0015750639140605927\n",
      "step = 259600: loss = 0.0010963963577523828\n",
      "step = 259800: loss = 0.0031342217698693275\n",
      "step = 260000: loss = 0.0056944722309708595\n",
      "step = 260000: Average Return = -200.0\n",
      "step = 260200: loss = 0.0017744374927133322\n",
      "step = 260400: loss = 0.005242004059255123\n",
      "step = 260600: loss = 0.0030623571947216988\n",
      "step = 260800: loss = 0.9338495135307312\n",
      "step = 261000: loss = 0.003275550901889801\n",
      "step = 261000: Average Return = -167.6999969482422\n",
      "step = 261200: loss = 0.012894492596387863\n",
      "step = 261400: loss = 0.002567931544035673\n",
      "step = 261600: loss = 0.003700926434248686\n",
      "step = 261800: loss = 1.0824291706085205\n",
      "step = 262000: loss = 0.020835030823946\n",
      "step = 262000: Average Return = -156.8000030517578\n",
      "step = 262200: loss = 0.0029280888848006725\n",
      "step = 262400: loss = 0.003921894356608391\n",
      "step = 262600: loss = 1.836771011352539\n",
      "step = 262800: loss = 0.004429014399647713\n",
      "step = 263000: loss = 0.008520036935806274\n",
      "step = 263000: Average Return = -200.0\n",
      "step = 263200: loss = 0.0048475307412445545\n",
      "step = 263400: loss = 1.1629992723464966\n",
      "step = 263600: loss = 0.003156710183247924\n",
      "step = 263800: loss = 0.003968340344727039\n",
      "step = 264000: loss = 0.0012443240266293287\n",
      "step = 264000: Average Return = -200.0\n",
      "step = 264200: loss = 1.5503474473953247\n",
      "step = 264400: loss = 0.0027271483559161425\n",
      "step = 264600: loss = 1.064164161682129\n",
      "step = 264800: loss = 1.0400062799453735\n",
      "step = 265000: loss = 0.0019423887133598328\n",
      "step = 265000: Average Return = -200.0\n",
      "step = 265200: loss = 0.0022644754499197006\n",
      "step = 265400: loss = 0.00432627135887742\n",
      "step = 265600: loss = 0.0016533921007066965\n",
      "step = 265800: loss = 0.004383825231343508\n",
      "step = 266000: loss = 1.131018042564392\n",
      "step = 266000: Average Return = -176.3000030517578\n",
      "step = 266200: loss = 1.1659481525421143\n",
      "step = 266400: loss = 0.00392540916800499\n",
      "step = 266600: loss = 0.00468336371704936\n",
      "step = 266800: loss = 1.0610085725784302\n",
      "step = 267000: loss = 1.0308364629745483\n",
      "step = 267000: Average Return = -170.0\n",
      "step = 267200: loss = 0.006004203576594591\n",
      "step = 267400: loss = 0.002446781611070037\n",
      "step = 267600: loss = 0.0023429745342582464\n",
      "step = 267800: loss = 1.0593096017837524\n",
      "step = 268000: loss = 0.010445614345371723\n",
      "step = 268000: Average Return = -136.0\n",
      "step = 268200: loss = 0.002878209576010704\n",
      "step = 268400: loss = 1.0325777530670166\n",
      "step = 268600: loss = 0.6233054995536804\n",
      "step = 268800: loss = 0.0095076197758317\n",
      "step = 269000: loss = 0.0014572395011782646\n",
      "step = 269000: Average Return = -144.6999969482422\n",
      "step = 269200: loss = 0.009345922619104385\n",
      "step = 269400: loss = 1.9686667919158936\n",
      "step = 269600: loss = 1.121768593788147\n",
      "step = 269800: loss = 2.2248005867004395\n",
      "step = 270000: loss = 1.0264294147491455\n",
      "step = 270000: Average Return = -200.0\n",
      "step = 270200: loss = 0.00235769129358232\n",
      "step = 270400: loss = 1.1530367136001587\n",
      "step = 270600: loss = 0.0031847278587520123\n",
      "step = 270800: loss = 0.0012815357185900211\n",
      "step = 271000: loss = 0.0042411694303154945\n",
      "step = 271000: Average Return = -133.39999389648438\n",
      "step = 271200: loss = 0.011828439310193062\n",
      "step = 271400: loss = 0.005358005408197641\n",
      "step = 271600: loss = 0.0018737164791673422\n",
      "step = 271800: loss = 0.0033708049450069666\n",
      "step = 272000: loss = 1.2062278985977173\n",
      "step = 272000: Average Return = -140.5\n",
      "step = 272200: loss = 0.001156990765593946\n",
      "step = 272400: loss = 1.1604238748550415\n",
      "step = 272600: loss = 0.005092448089271784\n",
      "step = 272800: loss = 0.003500510472804308\n",
      "step = 273000: loss = 0.004088640213012695\n",
      "step = 273000: Average Return = -190.89999389648438\n",
      "step = 273200: loss = 3.2067277431488037\n",
      "step = 273400: loss = 0.005616089329123497\n",
      "step = 273600: loss = 0.0013244845904409885\n",
      "step = 273800: loss = 0.001739368075504899\n",
      "step = 274000: loss = 0.006936141289770603\n",
      "step = 274000: Average Return = -200.0\n",
      "step = 274200: loss = 0.003956127446144819\n",
      "step = 274400: loss = 0.0030521831940859556\n",
      "step = 274600: loss = 1.0254487991333008\n",
      "step = 274800: loss = 0.0042641134932637215\n",
      "step = 275000: loss = 1.0235174894332886\n",
      "step = 275000: Average Return = -170.5\n",
      "step = 275200: loss = 0.006945649161934853\n",
      "step = 275400: loss = 0.0009595643496140838\n",
      "step = 275600: loss = 0.004764197859913111\n",
      "step = 275800: loss = 0.0024927444756031036\n",
      "step = 276000: loss = 0.0033948891796171665\n",
      "step = 276000: Average Return = -200.0\n",
      "step = 276200: loss = 0.9209690690040588\n",
      "step = 276400: loss = 0.00994125660508871\n",
      "step = 276600: loss = 0.0025848743971437216\n",
      "step = 276800: loss = 1.196913480758667\n",
      "step = 277000: loss = 0.006004177033901215\n",
      "step = 277000: Average Return = -200.0\n",
      "step = 277200: loss = 0.8378312587738037\n",
      "step = 277400: loss = 0.005089703947305679\n",
      "step = 277600: loss = 0.003520973026752472\n",
      "step = 277800: loss = 0.00419510155916214\n",
      "step = 278000: loss = 0.003883454017341137\n",
      "step = 278000: Average Return = -200.0\n",
      "step = 278200: loss = 0.8608174920082092\n",
      "step = 278400: loss = 0.010440757498145103\n",
      "step = 278600: loss = 0.00621411856263876\n",
      "step = 278800: loss = 1.0279148817062378\n",
      "step = 279000: loss = 0.005330648738890886\n",
      "step = 279000: Average Return = -200.0\n",
      "step = 279200: loss = 1.1506807804107666\n",
      "step = 279400: loss = 0.8324077725410461\n",
      "step = 279600: loss = 0.0036890655755996704\n",
      "step = 279800: loss = 0.0016366075724363327\n",
      "step = 280000: loss = 0.0052522821351885796\n",
      "step = 280000: Average Return = -131.0\n",
      "step = 280200: loss = 1.1257164478302002\n",
      "step = 280400: loss = 0.006029187701642513\n",
      "step = 280600: loss = 0.005233793053776026\n",
      "step = 280800: loss = 0.002157890470698476\n",
      "step = 281000: loss = 0.002322748303413391\n",
      "step = 281000: Average Return = -152.5\n",
      "step = 281200: loss = 0.002796787768602371\n",
      "step = 281400: loss = 1.056722640991211\n",
      "step = 281600: loss = 0.003655551001429558\n",
      "step = 281800: loss = 0.0038149587344378233\n",
      "step = 282000: loss = 0.00807322096079588\n",
      "step = 282000: Average Return = -136.39999389648438\n",
      "step = 282200: loss = 0.0032690241932868958\n",
      "step = 282400: loss = 0.0037278737872838974\n",
      "step = 282600: loss = 0.008042661473155022\n",
      "step = 282800: loss = 0.008164487779140472\n",
      "step = 283000: loss = 0.002131084678694606\n",
      "step = 283000: Average Return = -188.89999389648438\n",
      "step = 283200: loss = 0.004108566325157881\n",
      "step = 283400: loss = 1.1234667301177979\n",
      "step = 283600: loss = 0.004729584790766239\n",
      "step = 283800: loss = 0.0021123411133885384\n",
      "step = 284000: loss = 1.072695016860962\n",
      "step = 284000: Average Return = -200.0\n",
      "step = 284200: loss = 0.9773218035697937\n",
      "step = 284400: loss = 0.00607688631862402\n",
      "step = 284600: loss = 0.0038562798872590065\n",
      "step = 284800: loss = 0.01020798645913601\n",
      "step = 285000: loss = 0.9254659414291382\n",
      "step = 285000: Average Return = -200.0\n",
      "step = 285200: loss = 1.2186596393585205\n",
      "step = 285400: loss = 0.0016161164967343211\n",
      "step = 285600: loss = 0.0029590362682938576\n",
      "step = 285800: loss = 0.006212347187101841\n",
      "step = 286000: loss = 1.1233131885528564\n",
      "step = 286000: Average Return = -197.5\n",
      "step = 286200: loss = 0.003806064836680889\n",
      "step = 286400: loss = 0.8809646368026733\n",
      "step = 286600: loss = 0.013462497852742672\n",
      "step = 286800: loss = 2.034923553466797\n",
      "step = 287000: loss = 0.0027350778691470623\n",
      "step = 287000: Average Return = -200.0\n",
      "step = 287200: loss = 0.9191393852233887\n",
      "step = 287400: loss = 0.007858989760279655\n",
      "step = 287600: loss = 0.003117235377430916\n",
      "step = 287800: loss = 1.1456507444381714\n",
      "step = 288000: loss = 0.006227592937648296\n",
      "step = 288000: Average Return = -177.8000030517578\n",
      "step = 288200: loss = 1.241883635520935\n",
      "step = 288400: loss = 1.0655372142791748\n",
      "step = 288600: loss = 0.004125574603676796\n",
      "step = 288800: loss = 0.002343306317925453\n",
      "step = 289000: loss = 0.0032158582471311092\n",
      "step = 289000: Average Return = -200.0\n",
      "step = 289200: loss = 0.004895099438726902\n",
      "step = 289400: loss = 0.0026235764380544424\n",
      "step = 289600: loss = 1.0434514284133911\n",
      "step = 289800: loss = 0.0039818380028009415\n",
      "step = 290000: loss = 0.007130257785320282\n",
      "step = 290000: Average Return = -200.0\n",
      "step = 290200: loss = 0.0025183833204209805\n",
      "step = 290400: loss = 0.008417481556534767\n",
      "step = 290600: loss = 0.41923704743385315\n",
      "step = 290800: loss = 0.010640185326337814\n",
      "step = 291000: loss = 0.003941941075026989\n",
      "step = 291000: Average Return = -171.8000030517578\n",
      "step = 291200: loss = 0.008339451625943184\n",
      "step = 291400: loss = 0.003985155373811722\n",
      "step = 291600: loss = 0.003687735181301832\n",
      "step = 291800: loss = 0.0020110185723751783\n",
      "step = 292000: loss = 0.00773234898224473\n",
      "step = 292000: Average Return = -200.0\n",
      "step = 292200: loss = 0.007325645070523024\n",
      "step = 292400: loss = 0.00259092403575778\n",
      "step = 292600: loss = 0.631749153137207\n",
      "step = 292800: loss = 0.0022644789423793554\n",
      "step = 293000: loss = 2.249223232269287\n",
      "step = 293000: Average Return = -184.0\n",
      "step = 293200: loss = 0.003298377152532339\n",
      "step = 293400: loss = 0.002996502909809351\n",
      "step = 293600: loss = 0.008863409981131554\n",
      "step = 293800: loss = 0.0053117647767066956\n",
      "step = 294000: loss = 0.011070342734456062\n",
      "step = 294000: Average Return = -144.89999389648438\n",
      "step = 294200: loss = 0.001715951133519411\n",
      "step = 294400: loss = 0.0053031533025205135\n",
      "step = 294600: loss = 0.0037815910764038563\n",
      "step = 294800: loss = 0.0033113465178757906\n",
      "step = 295000: loss = 0.001362115959636867\n",
      "step = 295000: Average Return = -200.0\n",
      "step = 295200: loss = 0.00339434202760458\n",
      "step = 295400: loss = 0.6993536949157715\n",
      "step = 295600: loss = 0.9224228858947754\n",
      "step = 295800: loss = 0.8636411428451538\n",
      "step = 296000: loss = 0.0052666813135147095\n",
      "step = 296000: Average Return = -200.0\n",
      "step = 296200: loss = 0.0036297449842095375\n",
      "step = 296400: loss = 0.00377992307767272\n",
      "step = 296600: loss = 0.8231759667396545\n",
      "step = 296800: loss = 0.0024507518392056227\n",
      "step = 297000: loss = 0.00407068058848381\n",
      "step = 297000: Average Return = -200.0\n",
      "step = 297200: loss = 1.0817265510559082\n",
      "step = 297400: loss = 0.0060318405739963055\n",
      "step = 297600: loss = 0.006830804981291294\n",
      "step = 297800: loss = 0.9982437491416931\n",
      "step = 298000: loss = 0.009967956691980362\n",
      "step = 298000: Average Return = -199.60000610351562\n",
      "step = 298200: loss = 0.006361066363751888\n",
      "step = 298400: loss = 0.0028813406825065613\n",
      "step = 298600: loss = 0.005094973836094141\n",
      "step = 298800: loss = 0.7582743167877197\n",
      "step = 299000: loss = 0.0018870579078793526\n",
      "step = 299000: Average Return = -140.39999389648438\n",
      "step = 299200: loss = 1.0734407901763916\n",
      "step = 299400: loss = 0.012605362571775913\n",
      "step = 299600: loss = 0.7503001093864441\n",
      "step = 299800: loss = 0.6389163136482239\n",
      "step = 300000: loss = 0.005918875336647034\n",
      "step = 300000: Average Return = -192.10000610351562\n",
      "step = 300200: loss = 0.0032654914539307356\n",
      "step = 300400: loss = 0.00680050253868103\n",
      "step = 300600: loss = 1.9269911050796509\n",
      "step = 300800: loss = 1.1518464088439941\n",
      "step = 301000: loss = 0.0011597032425925136\n",
      "step = 301000: Average Return = -154.5\n",
      "step = 301200: loss = 1.3946592807769775\n",
      "step = 301400: loss = 0.00362634239718318\n",
      "step = 301600: loss = 0.0011683134362101555\n",
      "step = 301800: loss = 0.0017717434093356133\n",
      "step = 302000: loss = 0.0013414107961580157\n",
      "step = 302000: Average Return = -136.60000610351562\n",
      "step = 302200: loss = 0.0054747676476836205\n",
      "step = 302400: loss = 0.0034427433274686337\n",
      "step = 302600: loss = 0.004793894477188587\n",
      "step = 302800: loss = 0.0024596431758254766\n",
      "step = 303000: loss = 0.002623717999085784\n",
      "step = 303000: Average Return = -163.1999969482422\n",
      "step = 303200: loss = 1.1571522951126099\n",
      "step = 303400: loss = 0.005565572530031204\n",
      "step = 303600: loss = 0.004731975961476564\n",
      "step = 303800: loss = 1.0434420108795166\n",
      "step = 304000: loss = 1.0317745208740234\n",
      "step = 304000: Average Return = -200.0\n",
      "step = 304200: loss = 0.00421498715877533\n",
      "step = 304400: loss = 0.0077291084453463554\n",
      "step = 304600: loss = 0.9086447954177856\n",
      "step = 304800: loss = 0.0026834472082555294\n",
      "step = 305000: loss = 0.006305013783276081\n",
      "step = 305000: Average Return = -190.6999969482422\n",
      "step = 305200: loss = 0.006655341014266014\n",
      "step = 305400: loss = 1.106238842010498\n",
      "step = 305600: loss = 0.7654326558113098\n",
      "step = 305800: loss = 1.1069984436035156\n",
      "step = 306000: loss = 0.004991632886230946\n",
      "step = 306000: Average Return = -131.89999389648438\n",
      "step = 306200: loss = 1.1240507364273071\n",
      "step = 306400: loss = 1.1716458797454834\n",
      "step = 306600: loss = 1.1216474771499634\n",
      "step = 306800: loss = 0.007764332927763462\n",
      "step = 307000: loss = 0.003566827392205596\n",
      "step = 307000: Average Return = -200.0\n",
      "step = 307200: loss = 0.0018719189101830125\n",
      "step = 307400: loss = 0.012913409620523453\n",
      "step = 307600: loss = 0.0028072309214621782\n",
      "step = 307800: loss = 0.0026258034631609917\n",
      "step = 308000: loss = 1.1164419651031494\n",
      "step = 308000: Average Return = -160.39999389648438\n",
      "step = 308200: loss = 0.0036890001501888037\n",
      "step = 308400: loss = 0.002238848712295294\n",
      "step = 308600: loss = 1.0103543996810913\n",
      "step = 308800: loss = 0.683912456035614\n",
      "step = 309000: loss = 0.0025930367410182953\n",
      "step = 309000: Average Return = -193.10000610351562\n",
      "step = 309200: loss = 0.004798579029738903\n",
      "step = 309400: loss = 0.002624496351927519\n",
      "step = 309600: loss = 0.0017475951462984085\n",
      "step = 309800: loss = 0.0032088900916278362\n",
      "step = 310000: loss = 0.0065078772604465485\n",
      "step = 310000: Average Return = -187.1999969482422\n",
      "step = 310200: loss = 0.0025656200014054775\n",
      "step = 310400: loss = 0.00270909839309752\n",
      "step = 310600: loss = 0.0019752115476876497\n",
      "step = 310800: loss = 1.1433161497116089\n",
      "step = 311000: loss = 0.8249977827072144\n",
      "step = 311000: Average Return = -156.39999389648438\n",
      "step = 311200: loss = 0.01975734904408455\n",
      "step = 311400: loss = 1.4976081848144531\n",
      "step = 311600: loss = 0.6333971619606018\n",
      "step = 311800: loss = 0.007436580490320921\n",
      "step = 312000: loss = 0.0023964231368154287\n",
      "step = 312000: Average Return = -166.8000030517578\n",
      "step = 312200: loss = 0.841258704662323\n",
      "step = 312400: loss = 0.005873609334230423\n",
      "step = 312600: loss = 0.006302183493971825\n",
      "step = 312800: loss = 0.7950268983840942\n",
      "step = 313000: loss = 1.0453609228134155\n",
      "step = 313000: Average Return = -154.5\n",
      "step = 313200: loss = 1.0773558616638184\n",
      "step = 313400: loss = 0.0025742165744304657\n",
      "step = 313600: loss = 0.004676452372223139\n",
      "step = 313800: loss = 0.009041782468557358\n",
      "step = 314000: loss = 0.9719170331954956\n",
      "step = 314000: Average Return = -200.0\n",
      "step = 314200: loss = 0.0074782101437449455\n",
      "step = 314400: loss = 0.0019061657367274165\n",
      "step = 314600: loss = 1.12494695186615\n",
      "step = 314800: loss = 0.0041788918897509575\n",
      "step = 315000: loss = 0.005496528930962086\n",
      "step = 315000: Average Return = -154.1999969482422\n",
      "step = 315200: loss = 1.217106580734253\n",
      "step = 315400: loss = 0.005106927827000618\n",
      "step = 315600: loss = 1.0440374612808228\n",
      "step = 315800: loss = 0.0029156478121876717\n",
      "step = 316000: loss = 0.015618079341948032\n",
      "step = 316000: Average Return = -200.0\n",
      "step = 316200: loss = 0.007336238399147987\n",
      "step = 316400: loss = 0.8335669040679932\n",
      "step = 316600: loss = 0.004184698686003685\n",
      "step = 316800: loss = 0.005375510547310114\n",
      "step = 317000: loss = 1.1520359516143799\n",
      "step = 317000: Average Return = -149.8000030517578\n",
      "step = 317200: loss = 0.003022613935172558\n",
      "step = 317400: loss = 1.0191802978515625\n",
      "step = 317600: loss = 0.7001251578330994\n",
      "step = 317800: loss = 0.003713998245075345\n",
      "step = 318000: loss = 0.007174302823841572\n",
      "step = 318000: Average Return = -180.0\n",
      "step = 318200: loss = 0.0039799148216843605\n",
      "step = 318400: loss = 0.008057385683059692\n",
      "step = 318600: loss = 0.006736107636243105\n",
      "step = 318800: loss = 1.0483790636062622\n",
      "step = 319000: loss = 0.0030204844661056995\n",
      "step = 319000: Average Return = -142.6999969482422\n",
      "step = 319200: loss = 0.0011713530402630568\n",
      "step = 319400: loss = 0.0034158574417233467\n",
      "step = 319600: loss = 0.0029587941244244576\n",
      "step = 319800: loss = 0.004550052806735039\n",
      "step = 320000: loss = 0.007774649653583765\n",
      "step = 320000: Average Return = -200.0\n",
      "step = 320200: loss = 0.0024610513355582952\n",
      "step = 320400: loss = 0.535801351070404\n",
      "step = 320600: loss = 0.0022357143461704254\n",
      "step = 320800: loss = 0.005373815074563026\n",
      "step = 321000: loss = 0.9499576687812805\n",
      "step = 321000: Average Return = -161.8000030517578\n",
      "step = 321200: loss = 0.0028523982036858797\n",
      "step = 321400: loss = 0.0019715086091309786\n",
      "step = 321600: loss = 0.0055713895708322525\n",
      "step = 321800: loss = 0.69978266954422\n",
      "step = 322000: loss = 0.4086928963661194\n",
      "step = 322000: Average Return = -129.60000610351562\n",
      "step = 322200: loss = 1.027038335800171\n",
      "step = 322400: loss = 0.005910563748329878\n",
      "step = 322600: loss = 0.9103530049324036\n",
      "step = 322800: loss = 0.0054780165664851665\n",
      "step = 323000: loss = 1.1282382011413574\n",
      "step = 323000: Average Return = -200.0\n",
      "step = 323200: loss = 0.007327493280172348\n",
      "step = 323400: loss = 0.011141963303089142\n",
      "step = 323600: loss = 0.009823044762015343\n",
      "step = 323800: loss = 0.006895696744322777\n",
      "step = 324000: loss = 1.0297410488128662\n",
      "step = 324000: Average Return = -170.60000610351562\n",
      "step = 324200: loss = 0.913537323474884\n",
      "step = 324400: loss = 0.0023153084330260754\n",
      "step = 324600: loss = 0.8041619658470154\n",
      "step = 324800: loss = 1.0032671689987183\n",
      "step = 325000: loss = 0.004399427678436041\n",
      "step = 325000: Average Return = -163.39999389648438\n",
      "step = 325200: loss = 0.005306773353368044\n",
      "step = 325400: loss = 0.0023150499910116196\n",
      "step = 325600: loss = 1.0757087469100952\n",
      "step = 325800: loss = 0.007915426045656204\n",
      "step = 326000: loss = 0.014933296479284763\n",
      "step = 326000: Average Return = -170.0\n",
      "step = 326200: loss = 0.008758814074099064\n",
      "step = 326400: loss = 0.00626214686781168\n",
      "step = 326600: loss = 1.1054761409759521\n",
      "step = 326800: loss = 0.007388024125248194\n",
      "step = 327000: loss = 1.0110838413238525\n",
      "step = 327000: Average Return = -200.0\n",
      "step = 327200: loss = 0.9219693541526794\n",
      "step = 327400: loss = 0.007642987184226513\n",
      "step = 327600: loss = 0.0025613182224333286\n",
      "step = 327800: loss = 0.9149938821792603\n",
      "step = 328000: loss = 0.007137716747820377\n",
      "step = 328000: Average Return = -139.0\n",
      "step = 328200: loss = 0.006523555144667625\n",
      "step = 328400: loss = 0.0026424163952469826\n",
      "step = 328600: loss = 0.005386122036725283\n",
      "step = 328800: loss = 0.5304021835327148\n",
      "step = 329000: loss = 1.0945281982421875\n",
      "step = 329000: Average Return = -147.3000030517578\n",
      "step = 329200: loss = 0.004198620095849037\n",
      "step = 329400: loss = 1.0945590734481812\n",
      "step = 329600: loss = 0.0027925539761781693\n",
      "step = 329800: loss = 0.00272197974845767\n",
      "step = 330000: loss = 0.006300423759967089\n",
      "step = 330000: Average Return = -124.0999984741211\n",
      "step = 330200: loss = 1.1544371843338013\n",
      "step = 330400: loss = 0.003420347347855568\n",
      "step = 330600: loss = 0.0025145956315100193\n",
      "step = 330800: loss = 0.004073893651366234\n",
      "step = 331000: loss = 0.004571247845888138\n",
      "step = 331000: Average Return = -185.1999969482422\n",
      "step = 331200: loss = 1.153315544128418\n",
      "step = 331400: loss = 0.002572793047875166\n",
      "step = 331600: loss = 0.005282277707010508\n",
      "step = 331800: loss = 0.003264602040871978\n",
      "step = 332000: loss = 0.0038002347573637962\n",
      "step = 332000: Average Return = -127.69999694824219\n",
      "step = 332200: loss = 0.014390146359801292\n",
      "step = 332400: loss = 0.0034951847046613693\n",
      "step = 332600: loss = 0.0011821367079392076\n",
      "step = 332800: loss = 0.0020491923205554485\n",
      "step = 333000: loss = 2.099857807159424\n",
      "step = 333000: Average Return = -149.8000030517578\n",
      "step = 333200: loss = 1.1124364137649536\n",
      "step = 333400: loss = 0.6219791173934937\n",
      "step = 333600: loss = 0.007649750914424658\n",
      "step = 333800: loss = 0.9627377986907959\n",
      "step = 334000: loss = 0.0033989204093813896\n",
      "step = 334000: Average Return = -144.6999969482422\n",
      "step = 334200: loss = 0.004104721825569868\n",
      "step = 334400: loss = 1.1315841674804688\n",
      "step = 334600: loss = 1.057619333267212\n",
      "step = 334800: loss = 0.007355456240475178\n",
      "step = 335000: loss = 0.0017464609118178487\n",
      "step = 335000: Average Return = -122.30000305175781\n",
      "step = 335200: loss = 0.005206002853810787\n",
      "step = 335400: loss = 1.1303496360778809\n",
      "step = 335600: loss = 1.007026195526123\n",
      "step = 335800: loss = 0.005505415610969067\n",
      "step = 336000: loss = 0.0014067559968680143\n",
      "step = 336000: Average Return = -161.6999969482422\n",
      "step = 336200: loss = 0.00368245760910213\n",
      "step = 336400: loss = 0.002306392416357994\n",
      "step = 336600: loss = 0.005525253713130951\n",
      "step = 336800: loss = 0.004627274349331856\n",
      "step = 337000: loss = 0.8541457056999207\n",
      "step = 337000: Average Return = -135.60000610351562\n",
      "step = 337200: loss = 0.006436469033360481\n",
      "step = 337400: loss = 0.003314245492219925\n",
      "step = 337600: loss = 0.005114478059113026\n",
      "step = 337800: loss = 0.006118196062743664\n",
      "step = 338000: loss = 1.1215351819992065\n",
      "step = 338000: Average Return = -200.0\n",
      "step = 338200: loss = 0.009813610464334488\n",
      "step = 338400: loss = 0.00824245996773243\n",
      "step = 338600: loss = 0.002816091990098357\n",
      "step = 338800: loss = 0.027988916262984276\n",
      "step = 339000: loss = 0.003223470412194729\n",
      "step = 339000: Average Return = -200.0\n",
      "step = 339200: loss = 0.004583061672747135\n",
      "step = 339400: loss = 0.0034442455507814884\n",
      "step = 339600: loss = 0.002852225676178932\n",
      "step = 339800: loss = 0.0036565156187862158\n",
      "step = 340000: loss = 1.0434882640838623\n",
      "step = 340000: Average Return = -128.1999969482422\n",
      "step = 340200: loss = 0.0027579935267567635\n",
      "step = 340400: loss = 0.004708481952548027\n",
      "step = 340600: loss = 0.7345556616783142\n",
      "step = 340800: loss = 0.00542541453614831\n",
      "step = 341000: loss = 1.0710221529006958\n",
      "step = 341000: Average Return = -187.8000030517578\n",
      "step = 341200: loss = 0.006111220456659794\n",
      "step = 341400: loss = 0.003907834645360708\n",
      "step = 341600: loss = 0.007981117814779282\n",
      "step = 341800: loss = 0.012506723403930664\n",
      "step = 342000: loss = 0.0033062598668038845\n",
      "step = 342000: Average Return = -192.5\n",
      "step = 342200: loss = 0.005226060748100281\n",
      "step = 342400: loss = 0.006163472309708595\n",
      "step = 342600: loss = 0.01324634812772274\n",
      "step = 342800: loss = 1.1579533815383911\n",
      "step = 343000: loss = 1.1571638584136963\n",
      "step = 343000: Average Return = -171.3000030517578\n",
      "step = 343200: loss = 0.7003617882728577\n",
      "step = 343400: loss = 0.0021696470212191343\n",
      "step = 343600: loss = 0.003079580143094063\n",
      "step = 343800: loss = 1.0789858102798462\n",
      "step = 344000: loss = 0.0033843806013464928\n",
      "step = 344000: Average Return = -125.5999984741211\n",
      "step = 344200: loss = 0.0033899378031492233\n",
      "step = 344400: loss = 0.002725638449192047\n",
      "step = 344600: loss = 0.7788860201835632\n",
      "step = 344800: loss = 0.00635054474696517\n",
      "step = 345000: loss = 0.00500896479934454\n",
      "step = 345000: Average Return = -200.0\n",
      "step = 345200: loss = 0.0066385394893586636\n",
      "step = 345400: loss = 0.0039811283349990845\n",
      "step = 345600: loss = 0.005073514301329851\n",
      "step = 345800: loss = 0.016178008168935776\n",
      "step = 346000: loss = 0.004074205178767443\n",
      "step = 346000: Average Return = -152.89999389648438\n",
      "step = 346200: loss = 0.0038203003350645304\n",
      "step = 346400: loss = 0.011807535775005817\n",
      "step = 346600: loss = 0.013135177083313465\n",
      "step = 346800: loss = 0.004508165642619133\n",
      "step = 347000: loss = 0.004248227924108505\n",
      "step = 347000: Average Return = -176.0\n",
      "step = 347200: loss = 0.005209195427596569\n",
      "step = 347400: loss = 0.009886329993605614\n",
      "step = 347600: loss = 0.0031499213073402643\n",
      "step = 347800: loss = 0.0019983691163361073\n",
      "step = 348000: loss = 0.7749695181846619\n",
      "step = 348000: Average Return = -122.19999694824219\n",
      "step = 348200: loss = 0.007939929142594337\n",
      "step = 348400: loss = 0.006540152244269848\n",
      "step = 348600: loss = 0.002338474616408348\n",
      "step = 348800: loss = 0.4115155041217804\n",
      "step = 349000: loss = 0.9807392358779907\n",
      "step = 349000: Average Return = -124.30000305175781\n",
      "step = 349200: loss = 0.013109215535223484\n",
      "step = 349400: loss = 0.004342775791883469\n",
      "step = 349600: loss = 0.003113903570920229\n",
      "step = 349800: loss = 0.003230080474168062\n",
      "step = 350000: loss = 0.0027311446610838175\n",
      "step = 350000: Average Return = -149.0\n",
      "step = 350200: loss = 0.002441486343741417\n",
      "step = 350400: loss = 1.0326051712036133\n",
      "step = 350600: loss = 0.0035742460750043392\n",
      "step = 350800: loss = 0.0038765007629990578\n",
      "step = 351000: loss = 0.010107738897204399\n",
      "step = 351000: Average Return = -152.1999969482422\n",
      "step = 351200: loss = 0.7459366321563721\n",
      "step = 351400: loss = 0.004870990756899118\n",
      "step = 351600: loss = 0.003875984577462077\n",
      "step = 351800: loss = 0.0024068390484899282\n",
      "step = 352000: loss = 0.0034197992645204067\n",
      "step = 352000: Average Return = -171.10000610351562\n",
      "step = 352200: loss = 0.0028146558906883\n",
      "step = 352400: loss = 0.004582074470818043\n",
      "step = 352600: loss = 0.008369608782231808\n",
      "step = 352800: loss = 0.002033113967627287\n",
      "step = 353000: loss = 0.00474003329873085\n",
      "step = 353000: Average Return = -200.0\n",
      "step = 353200: loss = 0.017345527186989784\n",
      "step = 353400: loss = 0.0021878620609641075\n",
      "step = 353600: loss = 1.06952702999115\n",
      "step = 353800: loss = 1.0402987003326416\n",
      "step = 354000: loss = 2.1526684761047363\n",
      "step = 354000: Average Return = -200.0\n",
      "step = 354200: loss = 0.9075413346290588\n",
      "step = 354400: loss = 0.005885077640414238\n",
      "step = 354600: loss = 0.0065656909719109535\n",
      "step = 354800: loss = 1.2343454360961914\n",
      "step = 355000: loss = 1.22800874710083\n",
      "step = 355000: Average Return = -200.0\n",
      "step = 355200: loss = 0.024189230054616928\n",
      "step = 355400: loss = 0.006949455477297306\n",
      "step = 355600: loss = 0.005153518170118332\n",
      "step = 355800: loss = 0.929186999797821\n",
      "step = 356000: loss = 0.003601208794862032\n",
      "step = 356000: Average Return = -192.8000030517578\n",
      "step = 356200: loss = 0.0036962730810046196\n",
      "step = 356400: loss = 1.0317949056625366\n",
      "step = 356600: loss = 0.004399118945002556\n",
      "step = 356800: loss = 0.004647150635719299\n",
      "step = 357000: loss = 0.00448079127818346\n",
      "step = 357000: Average Return = -171.10000610351562\n",
      "step = 357200: loss = 0.01309908926486969\n",
      "step = 357400: loss = 0.003084097057580948\n",
      "step = 357600: loss = 0.002062416635453701\n",
      "step = 357800: loss = 0.0057967668399214745\n",
      "step = 358000: loss = 2.0012717247009277\n",
      "step = 358000: Average Return = -156.6999969482422\n",
      "step = 358200: loss = 0.8550742864608765\n",
      "step = 358400: loss = 0.01006716676056385\n",
      "step = 358600: loss = 0.004552319645881653\n",
      "step = 358800: loss = 0.7387234568595886\n",
      "step = 359000: loss = 1.0153921842575073\n",
      "step = 359000: Average Return = -129.60000610351562\n",
      "step = 359200: loss = 0.005368967540562153\n",
      "step = 359400: loss = 0.6601405143737793\n",
      "step = 359600: loss = 0.008139276877045631\n",
      "step = 359800: loss = 0.021623462438583374\n",
      "step = 360000: loss = 1.1248666048049927\n",
      "step = 360000: Average Return = -175.89999389648438\n",
      "step = 360200: loss = 0.0030921052675694227\n",
      "step = 360400: loss = 0.005329800769686699\n",
      "step = 360600: loss = 0.7249674797058105\n",
      "step = 360800: loss = 0.003041054354980588\n",
      "step = 361000: loss = 0.005445052869617939\n",
      "step = 361000: Average Return = -200.0\n",
      "step = 361200: loss = 0.7371982336044312\n",
      "step = 361400: loss = 0.0046057621948421\n",
      "step = 361600: loss = 0.02475842274725437\n",
      "step = 361800: loss = 0.009815741330385208\n",
      "step = 362000: loss = 0.00872815027832985\n",
      "step = 362000: Average Return = -156.0\n",
      "step = 362200: loss = 0.6728177070617676\n",
      "step = 362400: loss = 0.0032733017578721046\n",
      "step = 362600: loss = 0.00356679642572999\n",
      "step = 362800: loss = 0.006952712778002024\n",
      "step = 363000: loss = 1.9686169624328613\n",
      "step = 363000: Average Return = -170.60000610351562\n",
      "step = 363200: loss = 0.0052901366725564\n",
      "step = 363400: loss = 0.010293734259903431\n",
      "step = 363600: loss = 0.008919657208025455\n",
      "step = 363800: loss = 0.0024655768647789955\n",
      "step = 364000: loss = 0.004244605079293251\n",
      "step = 364000: Average Return = -200.0\n",
      "step = 364200: loss = 1.008548378944397\n",
      "step = 364400: loss = 0.0020699393935501575\n",
      "step = 364600: loss = 0.006624866276979446\n",
      "step = 364800: loss = 0.009411981329321861\n",
      "step = 365000: loss = 0.008919930085539818\n",
      "step = 365000: Average Return = -200.0\n",
      "step = 365200: loss = 0.39143335819244385\n",
      "step = 365400: loss = 0.003228261135518551\n",
      "step = 365600: loss = 0.007123068906366825\n",
      "step = 365800: loss = 1.1244637966156006\n",
      "step = 366000: loss = 0.002963679376989603\n",
      "step = 366000: Average Return = -200.0\n",
      "step = 366200: loss = 0.0035005214158445597\n",
      "step = 366400: loss = 0.0013657525414600968\n",
      "step = 366600: loss = 0.005249333567917347\n",
      "step = 366800: loss = 0.011478018946945667\n",
      "step = 367000: loss = 0.0024413932114839554\n",
      "step = 367000: Average Return = -199.5\n",
      "step = 367200: loss = 0.009303036145865917\n",
      "step = 367400: loss = 0.009536716155707836\n",
      "step = 367600: loss = 0.004131653346121311\n",
      "step = 367800: loss = 0.007388426922261715\n",
      "step = 368000: loss = 0.007336433045566082\n",
      "step = 368000: Average Return = -189.39999389648438\n",
      "step = 368200: loss = 0.006444273516535759\n",
      "step = 368400: loss = 0.9986559152603149\n",
      "step = 368600: loss = 0.9186428189277649\n",
      "step = 368800: loss = 0.0029027615673840046\n",
      "step = 369000: loss = 0.0017058169469237328\n",
      "step = 369000: Average Return = -200.0\n",
      "step = 369200: loss = 0.006574784405529499\n",
      "step = 369400: loss = 1.1799485683441162\n",
      "step = 369600: loss = 1.209160327911377\n",
      "step = 369800: loss = 1.2400429248809814\n",
      "step = 370000: loss = 0.00682735675945878\n",
      "step = 370000: Average Return = -139.10000610351562\n",
      "step = 370200: loss = 0.008416574448347092\n",
      "step = 370400: loss = 0.0030112690292298794\n",
      "step = 370600: loss = 0.006037277169525623\n",
      "step = 370800: loss = 0.014817624352872372\n",
      "step = 371000: loss = 0.008532872423529625\n",
      "step = 371000: Average Return = -200.0\n",
      "step = 371200: loss = 0.006604507565498352\n",
      "step = 371400: loss = 0.004860410466790199\n",
      "step = 371600: loss = 0.008672737516462803\n",
      "step = 371800: loss = 0.0028343237936496735\n",
      "step = 372000: loss = 0.01108541153371334\n",
      "step = 372000: Average Return = -200.0\n",
      "step = 372200: loss = 0.010226106271147728\n",
      "step = 372400: loss = 0.8056447505950928\n",
      "step = 372600: loss = 0.0042364089749753475\n",
      "step = 372800: loss = 0.004492097068578005\n",
      "step = 373000: loss = 0.003219433594495058\n",
      "step = 373000: Average Return = -200.0\n",
      "step = 373200: loss = 0.012806137092411518\n",
      "step = 373400: loss = 0.0027092653326690197\n",
      "step = 373600: loss = 0.7949578166007996\n",
      "step = 373800: loss = 0.0031154509633779526\n",
      "step = 374000: loss = 0.005157954059541225\n",
      "step = 374000: Average Return = -200.0\n",
      "step = 374200: loss = 0.7975096106529236\n",
      "step = 374400: loss = 2.2039072513580322\n",
      "step = 374600: loss = 0.006617710925638676\n",
      "step = 374800: loss = 0.00557304173707962\n",
      "step = 375000: loss = 1.0825681686401367\n",
      "step = 375000: Average Return = -165.10000610351562\n",
      "step = 375200: loss = 0.012873989529907703\n",
      "step = 375400: loss = 0.005216405261307955\n",
      "step = 375600: loss = 0.0023382152430713177\n",
      "step = 375800: loss = 1.0064046382904053\n",
      "step = 376000: loss = 0.0034934100694954395\n",
      "step = 376000: Average Return = -200.0\n",
      "step = 376200: loss = 0.0025635412894189358\n",
      "step = 376400: loss = 0.9483867883682251\n",
      "step = 376600: loss = 0.0026268628425896168\n",
      "step = 376800: loss = 0.003121328540146351\n",
      "step = 377000: loss = 0.0060762325301766396\n",
      "step = 377000: Average Return = -200.0\n",
      "step = 377200: loss = 0.009208066388964653\n",
      "step = 377400: loss = 0.6090992093086243\n",
      "step = 377600: loss = 1.8016724586486816\n",
      "step = 377800: loss = 0.002868582960218191\n",
      "step = 378000: loss = 0.0022555605974048376\n",
      "step = 378000: Average Return = -200.0\n",
      "step = 378200: loss = 1.0419517755508423\n",
      "step = 378400: loss = 0.0050324080511927605\n",
      "step = 378600: loss = 0.0054213907569646835\n",
      "step = 378800: loss = 0.7499986290931702\n",
      "step = 379000: loss = 1.0471951961517334\n",
      "step = 379000: Average Return = -200.0\n",
      "step = 379200: loss = 0.0033974580001085997\n",
      "step = 379400: loss = 0.003250124864280224\n",
      "step = 379600: loss = 0.620492160320282\n",
      "step = 379800: loss = 0.004829321056604385\n",
      "step = 380000: loss = 0.001749684801325202\n",
      "step = 380000: Average Return = -200.0\n",
      "step = 380200: loss = 0.0019863583147525787\n",
      "step = 380400: loss = 0.7865121364593506\n",
      "step = 380600: loss = 1.1266100406646729\n",
      "step = 380800: loss = 0.002743226708844304\n",
      "step = 381000: loss = 0.0017156809335574508\n",
      "step = 381000: Average Return = -178.3000030517578\n",
      "step = 381200: loss = 0.010655168443918228\n",
      "step = 381400: loss = 0.00672728568315506\n",
      "step = 381600: loss = 0.003953226841986179\n",
      "step = 381800: loss = 0.007287262938916683\n",
      "step = 382000: loss = 0.015379440039396286\n",
      "step = 382000: Average Return = -200.0\n",
      "step = 382200: loss = 0.002258279360830784\n",
      "step = 382400: loss = 0.009850623086094856\n",
      "step = 382600: loss = 0.0025232071056962013\n",
      "step = 382800: loss = 0.004533642902970314\n",
      "step = 383000: loss = 0.0033440692350268364\n",
      "step = 383000: Average Return = -200.0\n",
      "step = 383200: loss = 0.6982274651527405\n",
      "step = 383400: loss = 0.91179358959198\n",
      "step = 383600: loss = 0.0034318454563617706\n",
      "step = 383800: loss = 0.006359926424920559\n",
      "step = 384000: loss = 0.003086406271904707\n",
      "step = 384000: Average Return = -156.3000030517578\n",
      "step = 384200: loss = 0.8535429239273071\n",
      "step = 384400: loss = 0.005573899485170841\n",
      "step = 384600: loss = 0.00828151497989893\n",
      "step = 384800: loss = 0.0022382785100489855\n",
      "step = 385000: loss = 0.00216158339753747\n",
      "step = 385000: Average Return = -200.0\n",
      "step = 385200: loss = 1.302740454673767\n",
      "step = 385400: loss = 0.0038726290222257376\n",
      "step = 385600: loss = 0.613305926322937\n",
      "step = 385800: loss = 0.8680118322372437\n",
      "step = 386000: loss = 0.004798527806997299\n",
      "step = 386000: Average Return = -154.89999389648438\n",
      "step = 386200: loss = 0.008691812865436077\n",
      "step = 386400: loss = 0.015012187883257866\n",
      "step = 386600: loss = 0.006337559316307306\n",
      "step = 386800: loss = 0.8115803599357605\n",
      "step = 387000: loss = 0.0030057579278945923\n",
      "step = 387000: Average Return = -200.0\n",
      "step = 387200: loss = 0.0022803330793976784\n",
      "step = 387400: loss = 0.002990944776684046\n",
      "step = 387600: loss = 2.042215347290039\n",
      "step = 387800: loss = 0.003982280381023884\n",
      "step = 388000: loss = 0.007453245110809803\n",
      "step = 388000: Average Return = -200.0\n",
      "step = 388200: loss = 0.006603130139410496\n",
      "step = 388400: loss = 0.006325794383883476\n",
      "step = 388600: loss = 0.0029103895649313927\n",
      "step = 388800: loss = 0.006198408082127571\n",
      "step = 389000: loss = 0.006730446591973305\n",
      "step = 389000: Average Return = -200.0\n",
      "step = 389200: loss = 0.004034003708511591\n",
      "step = 389400: loss = 0.007310813758522272\n",
      "step = 389600: loss = 0.2394242137670517\n",
      "step = 389800: loss = 0.774451732635498\n",
      "step = 390000: loss = 0.010631216689944267\n",
      "step = 390000: Average Return = -200.0\n",
      "step = 390200: loss = 0.002966956701129675\n",
      "step = 390400: loss = 0.013375621289014816\n",
      "step = 390600: loss = 1.0531760454177856\n",
      "step = 390800: loss = 0.003532965201884508\n",
      "step = 391000: loss = 0.9082075953483582\n",
      "step = 391000: Average Return = -200.0\n",
      "step = 391200: loss = 0.005491310730576515\n",
      "step = 391400: loss = 0.0037949420511722565\n",
      "step = 391600: loss = 0.792863130569458\n",
      "step = 391800: loss = 1.0452375411987305\n",
      "step = 392000: loss = 0.9176449775695801\n",
      "step = 392000: Average Return = -200.0\n",
      "step = 392200: loss = 0.002333871554583311\n",
      "step = 392400: loss = 0.0038000294007360935\n",
      "step = 392600: loss = 0.010774443857371807\n",
      "step = 392800: loss = 0.005190803203731775\n",
      "step = 393000: loss = 0.006945891305804253\n",
      "step = 393000: Average Return = -192.3000030517578\n",
      "step = 393200: loss = 0.005741623230278492\n",
      "step = 393400: loss = 1.1680338382720947\n",
      "step = 393600: loss = 0.002724621444940567\n",
      "step = 393800: loss = 0.005445993505418301\n",
      "step = 394000: loss = 0.9796690940856934\n",
      "step = 394000: Average Return = -179.8000030517578\n",
      "step = 394200: loss = 0.008792182430624962\n",
      "step = 394400: loss = 0.008262615650892258\n",
      "step = 394600: loss = 0.007651114370673895\n",
      "step = 394800: loss = 1.8837178945541382\n",
      "step = 395000: loss = 0.010291267186403275\n",
      "step = 395000: Average Return = -200.0\n",
      "step = 395200: loss = 0.0038169817999005318\n",
      "step = 395400: loss = 0.003733447752892971\n",
      "step = 395600: loss = 1.9650449752807617\n",
      "step = 395800: loss = 0.006867579650133848\n",
      "step = 396000: loss = 0.002336943754926324\n",
      "step = 396000: Average Return = -200.0\n",
      "step = 396200: loss = 0.007176870480179787\n",
      "step = 396400: loss = 0.002220981754362583\n",
      "step = 396600: loss = 0.0028066816739737988\n",
      "step = 396800: loss = 0.008256364613771439\n",
      "step = 397000: loss = 0.006089668720960617\n",
      "step = 397000: Average Return = -168.39999389648438\n",
      "step = 397200: loss = 0.003631266998127103\n",
      "step = 397400: loss = 0.003432026831433177\n",
      "step = 397600: loss = 0.8459261059761047\n",
      "step = 397800: loss = 0.001798270270228386\n",
      "step = 398000: loss = 0.0039014569483697414\n",
      "step = 398000: Average Return = -200.0\n",
      "step = 398200: loss = 0.0035174055956304073\n",
      "step = 398400: loss = 0.9955014586448669\n",
      "step = 398600: loss = 0.887512743473053\n",
      "step = 398800: loss = 0.006223945878446102\n",
      "step = 399000: loss = 0.013750731013715267\n",
      "step = 399000: Average Return = -200.0\n",
      "step = 399200: loss = 0.0046968115493655205\n",
      "step = 399400: loss = 0.0028684991411864758\n",
      "step = 399600: loss = 0.004007586278021336\n",
      "step = 399800: loss = 0.021633006632328033\n",
      "step = 400000: loss = 0.004438907839357853\n",
      "step = 400000: Average Return = -176.1999969482422\n",
      "step = 400200: loss = 0.0020406250841915607\n",
      "step = 400400: loss = 0.002757130889222026\n",
      "step = 400600: loss = 0.0052700466476380825\n",
      "step = 400800: loss = 0.005081448704004288\n",
      "step = 401000: loss = 2.1650891304016113\n",
      "step = 401000: Average Return = -200.0\n",
      "step = 401200: loss = 1.2206814289093018\n",
      "step = 401400: loss = 0.9412142634391785\n",
      "step = 401600: loss = 0.004541601054370403\n",
      "step = 401800: loss = 1.6778571605682373\n",
      "step = 402000: loss = 0.007317523472011089\n",
      "step = 402000: Average Return = -178.60000610351562\n",
      "step = 402200: loss = 0.920312762260437\n",
      "step = 402400: loss = 0.0077400729060173035\n",
      "step = 402600: loss = 0.00442718667909503\n",
      "step = 402800: loss = 0.008082419633865356\n",
      "step = 403000: loss = 0.013100294396281242\n",
      "step = 403000: Average Return = -156.0\n",
      "step = 403200: loss = 0.006991896312683821\n",
      "step = 403400: loss = 1.7969367504119873\n",
      "step = 403600: loss = 0.0020468749571591616\n",
      "step = 403800: loss = 0.001195334829390049\n",
      "step = 404000: loss = 0.8382652997970581\n",
      "step = 404000: Average Return = -200.0\n",
      "step = 404200: loss = 1.023483157157898\n",
      "step = 404400: loss = 0.007728801108896732\n",
      "step = 404600: loss = 0.003088134340941906\n",
      "step = 404800: loss = 0.5888168811798096\n",
      "step = 405000: loss = 0.0027097666170448065\n",
      "step = 405000: Average Return = -151.5\n",
      "step = 405200: loss = 0.008001076057553291\n",
      "step = 405400: loss = 0.011859462596476078\n",
      "step = 405600: loss = 0.00252745533362031\n",
      "step = 405800: loss = 0.7865859270095825\n",
      "step = 406000: loss = 0.002027468755841255\n",
      "step = 406000: Average Return = -171.3000030517578\n",
      "step = 406200: loss = 0.00917134527117014\n",
      "step = 406400: loss = 0.0038887555710971355\n",
      "step = 406600: loss = 0.0025844001211225986\n",
      "step = 406800: loss = 0.5198891758918762\n",
      "step = 407000: loss = 0.0020878571085631847\n",
      "step = 407000: Average Return = -200.0\n",
      "step = 407200: loss = 0.0033743686508387327\n",
      "step = 407400: loss = 0.003406826639547944\n",
      "step = 407600: loss = 0.006336159538477659\n",
      "step = 407800: loss = 0.006435813382267952\n",
      "step = 408000: loss = 0.9296668767929077\n",
      "step = 408000: Average Return = -146.1999969482422\n",
      "step = 408200: loss = 0.00618004659190774\n",
      "step = 408400: loss = 0.0031694374047219753\n",
      "step = 408600: loss = 0.010439250618219376\n",
      "step = 408800: loss = 0.009497085586190224\n",
      "step = 409000: loss = 0.7717493176460266\n",
      "step = 409000: Average Return = -195.3000030517578\n",
      "step = 409200: loss = 0.0031656285282224417\n",
      "step = 409400: loss = 0.015754977241158485\n",
      "step = 409600: loss = 0.006303371395915747\n",
      "step = 409800: loss = 0.9966462850570679\n",
      "step = 410000: loss = 0.6465839147567749\n",
      "step = 410000: Average Return = -200.0\n",
      "step = 410200: loss = 1.0742058753967285\n",
      "step = 410400: loss = 0.0050231413915753365\n",
      "step = 410600: loss = 0.8075154423713684\n",
      "step = 410800: loss = 0.5875348448753357\n",
      "step = 411000: loss = 0.011298148892819881\n",
      "step = 411000: Average Return = -187.10000610351562\n",
      "step = 411200: loss = 1.1542880535125732\n",
      "step = 411400: loss = 1.6619116067886353\n",
      "step = 411600: loss = 0.007588131353259087\n",
      "step = 411800: loss = 0.004301637876778841\n",
      "step = 412000: loss = 0.005958601366728544\n",
      "step = 412000: Average Return = -185.0\n",
      "step = 412200: loss = 0.005494153592735529\n",
      "step = 412400: loss = 0.003382077906280756\n",
      "step = 412600: loss = 0.02386593446135521\n",
      "step = 412800: loss = 0.004430030006915331\n",
      "step = 413000: loss = 0.004011507611721754\n",
      "step = 413000: Average Return = -180.6999969482422\n",
      "step = 413200: loss = 0.003920014947652817\n",
      "step = 413400: loss = 0.003522356739267707\n",
      "step = 413600: loss = 0.0072998348623514175\n",
      "step = 413800: loss = 0.0017965772422030568\n",
      "step = 414000: loss = 0.004232901148498058\n",
      "step = 414000: Average Return = -200.0\n",
      "step = 414200: loss = 0.004513503517955542\n",
      "step = 414400: loss = 0.003793236566707492\n",
      "step = 414600: loss = 0.003826187690719962\n",
      "step = 414800: loss = 0.0054547423496842384\n",
      "step = 415000: loss = 0.004333684220910072\n",
      "step = 415000: Average Return = -200.0\n",
      "step = 415200: loss = 0.0072313821874558926\n",
      "step = 415400: loss = 0.0032537984661757946\n",
      "step = 415600: loss = 0.0036647075321525335\n",
      "step = 415800: loss = 0.003040306270122528\n",
      "step = 416000: loss = 0.0037297408562153578\n",
      "step = 416000: Average Return = -200.0\n",
      "step = 416200: loss = 0.0013866208028048277\n",
      "step = 416400: loss = 0.0025732542853802443\n",
      "step = 416600: loss = 0.0022691329941153526\n",
      "step = 416800: loss = 0.01380779966711998\n",
      "step = 417000: loss = 0.0017370335990563035\n",
      "step = 417000: Average Return = -200.0\n",
      "step = 417200: loss = 0.011822721920907497\n",
      "step = 417400: loss = 0.005333751440048218\n",
      "step = 417600: loss = 0.003827029373496771\n",
      "step = 417800: loss = 0.004073571413755417\n",
      "step = 418000: loss = 1.0143723487854004\n",
      "step = 418000: Average Return = -200.0\n",
      "step = 418200: loss = 0.007951894775032997\n",
      "step = 418400: loss = 0.9416168928146362\n",
      "step = 418600: loss = 2.2778069972991943\n",
      "step = 418800: loss = 1.056673288345337\n",
      "step = 419000: loss = 0.0020645312033593655\n",
      "step = 419000: Average Return = -148.1999969482422\n",
      "step = 419200: loss = 1.7548326253890991\n",
      "step = 419400: loss = 0.005093425512313843\n",
      "step = 419600: loss = 0.004980631172657013\n",
      "step = 419800: loss = 0.008052239194512367\n",
      "step = 420000: loss = 0.00545417470857501\n",
      "step = 420000: Average Return = -200.0\n",
      "step = 420200: loss = 0.021656205877661705\n",
      "step = 420400: loss = 0.005556365475058556\n",
      "step = 420600: loss = 1.2328459024429321\n",
      "step = 420800: loss = 0.004870180040597916\n",
      "step = 421000: loss = 0.19582052528858185\n",
      "step = 421000: Average Return = -154.89999389648438\n",
      "step = 421200: loss = 0.006737401708960533\n",
      "step = 421400: loss = 2.2061030864715576\n",
      "step = 421600: loss = 1.013503074645996\n",
      "step = 421800: loss = 0.0032941445242613554\n",
      "step = 422000: loss = 0.005339473951607943\n",
      "step = 422000: Average Return = -200.0\n",
      "step = 422200: loss = 0.007314764894545078\n",
      "step = 422400: loss = 0.0016283616423606873\n",
      "step = 422600: loss = 0.9384580254554749\n",
      "step = 422800: loss = 0.7435619235038757\n",
      "step = 423000: loss = 0.006329418625682592\n",
      "step = 423000: Average Return = -200.0\n",
      "step = 423200: loss = 1.1195712089538574\n",
      "step = 423400: loss = 0.007237153593450785\n",
      "step = 423600: loss = 1.2099138498306274\n",
      "step = 423800: loss = 0.004315427038818598\n",
      "step = 424000: loss = 0.007268507033586502\n",
      "step = 424000: Average Return = -197.39999389648438\n",
      "step = 424200: loss = 0.0017801049398258328\n",
      "step = 424400: loss = 0.012041069567203522\n",
      "step = 424600: loss = 0.007625628262758255\n",
      "step = 424800: loss = 0.006201655603945255\n",
      "step = 425000: loss = 0.8746292591094971\n",
      "step = 425000: Average Return = -117.9000015258789\n",
      "step = 425200: loss = 0.6769622564315796\n",
      "step = 425400: loss = 1.101981520652771\n",
      "step = 425600: loss = 0.005382883828133345\n",
      "step = 425800: loss = 0.9388497471809387\n",
      "step = 426000: loss = 0.006187018007040024\n",
      "step = 426000: Average Return = -200.0\n",
      "step = 426200: loss = 0.0016336186090484262\n",
      "step = 426400: loss = 0.005336673930287361\n",
      "step = 426600: loss = 0.01445319689810276\n",
      "step = 426800: loss = 1.8272291421890259\n",
      "step = 427000: loss = 0.002420032164081931\n",
      "step = 427000: Average Return = -157.5\n",
      "step = 427200: loss = 0.014268080703914165\n",
      "step = 427400: loss = 0.004697403404861689\n",
      "step = 427600: loss = 0.8768998384475708\n",
      "step = 427800: loss = 0.0059904600493609905\n",
      "step = 428000: loss = 0.002940715989097953\n",
      "step = 428000: Average Return = -153.3000030517578\n",
      "step = 428200: loss = 0.7773597240447998\n",
      "step = 428400: loss = 0.013095390051603317\n",
      "step = 428600: loss = 1.788728952407837\n",
      "step = 428800: loss = 0.9614128470420837\n",
      "step = 429000: loss = 0.7921947240829468\n",
      "step = 429000: Average Return = -200.0\n",
      "step = 429200: loss = 0.006554673425853252\n",
      "step = 429400: loss = 0.009270701557397842\n",
      "step = 429600: loss = 0.008828433230519295\n",
      "step = 429800: loss = 0.014794763177633286\n",
      "step = 430000: loss = 0.6633810997009277\n",
      "step = 430000: Average Return = -180.3000030517578\n",
      "step = 430200: loss = 0.003971027676016092\n",
      "step = 430400: loss = 0.003735130652785301\n",
      "step = 430600: loss = 0.004844199400395155\n",
      "step = 430800: loss = 0.01759074255824089\n",
      "step = 431000: loss = 0.016124505549669266\n",
      "step = 431000: Average Return = -200.0\n",
      "step = 431200: loss = 0.008426717482507229\n",
      "step = 431400: loss = 0.0019924782682210207\n",
      "step = 431600: loss = 1.2353140115737915\n",
      "step = 431800: loss = 0.007316203322261572\n",
      "step = 432000: loss = 0.00333526823669672\n",
      "step = 432000: Average Return = -200.0\n",
      "step = 432200: loss = 2.3152124881744385\n",
      "step = 432400: loss = 0.00239420123398304\n",
      "step = 432600: loss = 0.007371827028691769\n",
      "step = 432800: loss = 0.00162924500182271\n",
      "step = 433000: loss = 0.004929827526211739\n",
      "step = 433000: Average Return = -175.3000030517578\n",
      "step = 433200: loss = 0.001954387640580535\n",
      "step = 433400: loss = 0.008988863788545132\n",
      "step = 433600: loss = 0.005771859548985958\n",
      "step = 433800: loss = 0.0015975894639268517\n",
      "step = 434000: loss = 1.2109750509262085\n",
      "step = 434000: Average Return = -148.39999389648438\n",
      "step = 434200: loss = 0.006768419407308102\n",
      "step = 434400: loss = 0.012553734704852104\n",
      "step = 434600: loss = 1.0890260934829712\n",
      "step = 434800: loss = 0.40837275981903076\n",
      "step = 435000: loss = 0.0022328533232212067\n",
      "step = 435000: Average Return = -200.0\n",
      "step = 435200: loss = 0.01783852092921734\n",
      "step = 435400: loss = 0.006579984910786152\n",
      "step = 435600: loss = 1.2439244985580444\n",
      "step = 435800: loss = 1.0023092031478882\n",
      "step = 436000: loss = 0.0032798899337649345\n",
      "step = 436000: Average Return = -143.3000030517578\n",
      "step = 436200: loss = 0.01035221479833126\n",
      "step = 436400: loss = 0.006676308810710907\n",
      "step = 436600: loss = 0.003809461137279868\n",
      "step = 436800: loss = 0.008818427100777626\n",
      "step = 437000: loss = 0.0032973377965390682\n",
      "step = 437000: Average Return = -160.0\n",
      "step = 437200: loss = 0.004363922867923975\n",
      "step = 437400: loss = 1.2189637422561646\n",
      "step = 437600: loss = 0.008419506251811981\n",
      "step = 437800: loss = 0.0030114692635834217\n",
      "step = 438000: loss = 1.3954567909240723\n",
      "step = 438000: Average Return = -200.0\n",
      "step = 438200: loss = 1.107709527015686\n",
      "step = 438400: loss = 0.010575023479759693\n",
      "step = 438600: loss = 0.0032584278378635645\n",
      "step = 438800: loss = 1.091565489768982\n",
      "step = 439000: loss = 0.00502421148121357\n",
      "step = 439000: Average Return = -200.0\n",
      "step = 439200: loss = 0.005313204601407051\n",
      "step = 439400: loss = 0.0021888641640543938\n",
      "step = 439600: loss = 0.0029642826411873102\n",
      "step = 439800: loss = 2.108959674835205\n",
      "step = 440000: loss = 0.004787403158843517\n",
      "step = 440000: Average Return = -174.6999969482422\n",
      "step = 440200: loss = 0.0039727333933115005\n",
      "step = 440400: loss = 0.003524682018905878\n",
      "step = 440600: loss = 1.715376377105713\n",
      "step = 440800: loss = 0.004346766509115696\n",
      "step = 441000: loss = 0.002325203036889434\n",
      "step = 441000: Average Return = -200.0\n",
      "step = 441200: loss = 1.076723337173462\n",
      "step = 441400: loss = 0.0023882719688117504\n",
      "step = 441600: loss = 0.00634203152731061\n",
      "step = 441800: loss = 0.006525096949189901\n",
      "step = 442000: loss = 0.004539829678833485\n",
      "step = 442000: Average Return = -200.0\n",
      "step = 442200: loss = 0.006497572176158428\n",
      "step = 442400: loss = 0.7894277572631836\n",
      "step = 442600: loss = 0.9664413332939148\n",
      "step = 442800: loss = 1.2358484268188477\n",
      "step = 443000: loss = 0.002666918560862541\n",
      "step = 443000: Average Return = -178.10000610351562\n",
      "step = 443200: loss = 0.00342927360907197\n",
      "step = 443400: loss = 0.0014513389905914664\n",
      "step = 443600: loss = 1.2296969890594482\n",
      "step = 443800: loss = 0.0031801266595721245\n",
      "step = 444000: loss = 0.008011612109839916\n",
      "step = 444000: Average Return = -198.1999969482422\n",
      "step = 444200: loss = 0.6977241635322571\n",
      "step = 444400: loss = 0.0106583833694458\n",
      "step = 444600: loss = 1.128236174583435\n",
      "step = 444800: loss = 1.092807412147522\n",
      "step = 445000: loss = 0.002740576397627592\n",
      "step = 445000: Average Return = -176.60000610351562\n",
      "step = 445200: loss = 1.078694462776184\n",
      "step = 445400: loss = 1.2453218698501587\n",
      "step = 445600: loss = 1.138461947441101\n",
      "step = 445800: loss = 0.0037797517143189907\n",
      "step = 446000: loss = 0.0036818720400333405\n",
      "step = 446000: Average Return = -136.5\n",
      "step = 446200: loss = 1.0065339803695679\n",
      "step = 446400: loss = 1.7004363536834717\n",
      "step = 446600: loss = 0.007029752712696791\n",
      "step = 446800: loss = 1.2086241245269775\n",
      "step = 447000: loss = 0.0016462744679301977\n",
      "step = 447000: Average Return = -200.0\n",
      "step = 447200: loss = 0.002642908599227667\n",
      "step = 447400: loss = 0.006184431724250317\n",
      "step = 447600: loss = 0.0017918728990480304\n",
      "step = 447800: loss = 0.0050822957418859005\n",
      "step = 448000: loss = 0.00357453478500247\n",
      "step = 448000: Average Return = -162.0\n",
      "step = 448200: loss = 0.0029809419065713882\n",
      "step = 448400: loss = 0.007178857922554016\n",
      "step = 448600: loss = 0.007557135075330734\n",
      "step = 448800: loss = 0.009503464214503765\n",
      "step = 449000: loss = 0.0020214077085256577\n",
      "step = 449000: Average Return = -149.0\n",
      "step = 449200: loss = 1.0500510931015015\n",
      "step = 449400: loss = 0.0018163203494623303\n",
      "step = 449600: loss = 0.004939394071698189\n",
      "step = 449800: loss = 0.010925289243459702\n",
      "step = 450000: loss = 0.0033099078573286533\n",
      "step = 450000: Average Return = -183.6999969482422\n",
      "step = 450200: loss = 0.8484203815460205\n",
      "step = 450400: loss = 0.0022186841815710068\n",
      "step = 450600: loss = 0.008839625865221024\n",
      "step = 450800: loss = 0.0028507914394140244\n",
      "step = 451000: loss = 0.003844218095764518\n",
      "step = 451000: Average Return = -116.5\n",
      "step = 451200: loss = 0.008531973697245121\n",
      "step = 451400: loss = 0.0037781978026032448\n",
      "step = 451600: loss = 0.0033083355519920588\n",
      "step = 451800: loss = 2.2471818923950195\n",
      "step = 452000: loss = 0.7522742748260498\n",
      "step = 452000: Average Return = -200.0\n",
      "step = 452200: loss = 1.153469443321228\n",
      "step = 452400: loss = 0.002347702858969569\n",
      "step = 452600: loss = 0.0028819777071475983\n",
      "step = 452800: loss = 0.0031725375447422266\n",
      "step = 453000: loss = 0.9695355296134949\n",
      "step = 453000: Average Return = -143.60000610351562\n",
      "step = 453200: loss = 0.6663915514945984\n",
      "step = 453400: loss = 0.0031100912019610405\n",
      "step = 453600: loss = 1.2406635284423828\n",
      "step = 453800: loss = 0.003891878994181752\n",
      "step = 454000: loss = 0.008653072640299797\n",
      "step = 454000: Average Return = -200.0\n",
      "step = 454200: loss = 0.007427511736750603\n",
      "step = 454400: loss = 0.0018873735098168254\n",
      "step = 454600: loss = 0.005932037252932787\n",
      "step = 454800: loss = 0.005020598415285349\n",
      "step = 455000: loss = 0.003665817901492119\n",
      "step = 455000: Average Return = -191.8000030517578\n",
      "step = 455200: loss = 0.0040717581287026405\n",
      "step = 455400: loss = 0.9079241752624512\n",
      "step = 455600: loss = 0.00144304393325001\n",
      "step = 455800: loss = 1.0770684480667114\n",
      "step = 456000: loss = 0.006490772590041161\n",
      "step = 456000: Average Return = -173.3000030517578\n",
      "step = 456200: loss = 0.005210652016103268\n",
      "step = 456400: loss = 0.013215707615017891\n",
      "step = 456600: loss = 0.0037553843576461077\n",
      "step = 456800: loss = 1.247732400894165\n",
      "step = 457000: loss = 0.020009063184261322\n",
      "step = 457000: Average Return = -177.3000030517578\n",
      "step = 457200: loss = 0.0043156445026397705\n",
      "step = 457400: loss = 0.004476234782487154\n",
      "step = 457600: loss = 0.7540948987007141\n",
      "step = 457800: loss = 0.007073627784848213\n",
      "step = 458000: loss = 0.009275155141949654\n",
      "step = 458000: Average Return = -184.0\n",
      "step = 458200: loss = 0.008811252191662788\n",
      "step = 458400: loss = 0.003926530014723539\n",
      "step = 458600: loss = 0.005452772602438927\n",
      "step = 458800: loss = 0.002808533376082778\n",
      "step = 459000: loss = 0.0021889950148761272\n",
      "step = 459000: Average Return = -185.8000030517578\n",
      "step = 459200: loss = 0.005693632178008556\n",
      "step = 459400: loss = 0.0041336361318826675\n",
      "step = 459600: loss = 0.003506641136482358\n",
      "step = 459800: loss = 0.006161665543913841\n",
      "step = 460000: loss = 0.0051830061711370945\n",
      "step = 460000: Average Return = -200.0\n",
      "step = 460200: loss = 0.0031055263243615627\n",
      "step = 460400: loss = 0.005181053653359413\n",
      "step = 460600: loss = 0.0028395499102771282\n",
      "step = 460800: loss = 0.01058810856193304\n",
      "step = 461000: loss = 0.004416395910084248\n",
      "step = 461000: Average Return = -132.8000030517578\n",
      "step = 461200: loss = 2.156818389892578\n",
      "step = 461400: loss = 0.003969977609813213\n",
      "step = 461600: loss = 0.005149819888174534\n",
      "step = 461800: loss = 0.00267165363766253\n",
      "step = 462000: loss = 0.0022012086119502783\n",
      "step = 462000: Average Return = -185.8000030517578\n",
      "step = 462200: loss = 0.9019054770469666\n",
      "step = 462400: loss = 0.0054589021019637585\n",
      "step = 462600: loss = 1.224295735359192\n",
      "step = 462800: loss = 0.004838766995817423\n",
      "step = 463000: loss = 0.9859701991081238\n",
      "step = 463000: Average Return = -200.0\n",
      "step = 463200: loss = 0.002709045074880123\n",
      "step = 463400: loss = 0.6185355186462402\n",
      "step = 463600: loss = 0.015987956896424294\n",
      "step = 463800: loss = 0.00340642174705863\n",
      "step = 464000: loss = 0.009148867800831795\n",
      "step = 464000: Average Return = -180.60000610351562\n",
      "step = 464200: loss = 0.006228974089026451\n",
      "step = 464400: loss = 0.007466468960046768\n",
      "step = 464600: loss = 0.00390632776543498\n",
      "step = 464800: loss = 0.011574256233870983\n",
      "step = 465000: loss = 0.003409050637856126\n",
      "step = 465000: Average Return = -200.0\n",
      "step = 465200: loss = 0.0022984163369983435\n",
      "step = 465400: loss = 0.011983497068285942\n",
      "step = 465600: loss = 0.0026522858534008265\n",
      "step = 465800: loss = 0.005177210550755262\n",
      "step = 466000: loss = 0.001899047987535596\n",
      "step = 466000: Average Return = -194.0\n",
      "step = 466200: loss = 1.1555111408233643\n",
      "step = 466400: loss = 0.006164228543639183\n",
      "step = 466600: loss = 0.003243011422455311\n",
      "step = 466800: loss = 0.002764799166470766\n",
      "step = 467000: loss = 0.004148677922785282\n",
      "step = 467000: Average Return = -200.0\n",
      "step = 467200: loss = 0.6699427962303162\n",
      "step = 467400: loss = 0.0028502088971436024\n",
      "step = 467600: loss = 0.6594743728637695\n",
      "step = 467800: loss = 0.002904796041548252\n",
      "step = 468000: loss = 0.00315756443887949\n",
      "step = 468000: Average Return = -200.0\n",
      "step = 468200: loss = 0.9581823945045471\n",
      "step = 468400: loss = 0.9884598255157471\n",
      "step = 468600: loss = 0.0037162976805120707\n",
      "step = 468800: loss = 0.002976727904751897\n",
      "step = 469000: loss = 0.005111736245453358\n",
      "step = 469000: Average Return = -174.1999969482422\n",
      "step = 469200: loss = 1.0664782524108887\n",
      "step = 469400: loss = 0.005870404653251171\n",
      "step = 469600: loss = 0.004938711877912283\n",
      "step = 469800: loss = 0.6224391460418701\n",
      "step = 470000: loss = 0.0017128584440797567\n",
      "step = 470000: Average Return = -184.6999969482422\n",
      "step = 470200: loss = 0.6204872131347656\n",
      "step = 470400: loss = 0.004408101551234722\n",
      "step = 470600: loss = 0.0034644128754734993\n",
      "step = 470800: loss = 0.00197886535897851\n",
      "step = 471000: loss = 0.003427606076002121\n",
      "step = 471000: Average Return = -200.0\n",
      "step = 471200: loss = 0.0018449381459504366\n",
      "step = 471400: loss = 0.00493360310792923\n",
      "step = 471600: loss = 0.008071672171354294\n",
      "step = 471800: loss = 0.020736562088131905\n",
      "step = 472000: loss = 0.009896715171635151\n",
      "step = 472000: Average Return = -158.8000030517578\n",
      "step = 472200: loss = 0.003912352491170168\n",
      "step = 472400: loss = 0.003983928821980953\n",
      "step = 472600: loss = 1.1620827913284302\n",
      "step = 472800: loss = 2.163823127746582\n",
      "step = 473000: loss = 1.0591453313827515\n",
      "step = 473000: Average Return = -158.5\n",
      "step = 473200: loss = 0.004175169393420219\n",
      "step = 473400: loss = 0.0026462837122380733\n",
      "step = 473600: loss = 0.0018607106758281589\n",
      "step = 473800: loss = 0.6872481107711792\n",
      "step = 474000: loss = 0.004791898652911186\n",
      "step = 474000: Average Return = -200.0\n",
      "step = 474200: loss = 0.002112947404384613\n",
      "step = 474400: loss = 0.007101333700120449\n",
      "step = 474600: loss = 0.003744391957297921\n",
      "step = 474800: loss = 0.002160199685022235\n",
      "step = 475000: loss = 0.008569257333874702\n",
      "step = 475000: Average Return = -172.89999389648438\n",
      "step = 475200: loss = 0.004015445243567228\n",
      "step = 475400: loss = 1.008152723312378\n",
      "step = 475600: loss = 1.1596423387527466\n",
      "step = 475800: loss = 0.9823158979415894\n",
      "step = 476000: loss = 0.009050754830241203\n",
      "step = 476000: Average Return = -200.0\n",
      "step = 476200: loss = 1.071096658706665\n",
      "step = 476400: loss = 0.8754239082336426\n",
      "step = 476600: loss = 0.006198931485414505\n",
      "step = 476800: loss = 0.0014761099591851234\n",
      "step = 477000: loss = 1.2355061769485474\n",
      "step = 477000: Average Return = -150.10000610351562\n",
      "step = 477200: loss = 0.005226035602390766\n",
      "step = 477400: loss = 1.147707223892212\n",
      "step = 477600: loss = 1.5286415815353394\n",
      "step = 477800: loss = 0.005438821390271187\n",
      "step = 478000: loss = 0.003629488404840231\n",
      "step = 478000: Average Return = -200.0\n",
      "step = 478200: loss = 1.7306357622146606\n",
      "step = 478400: loss = 0.007705391384661198\n",
      "step = 478600: loss = 0.8464885354042053\n",
      "step = 478800: loss = 0.002174733206629753\n",
      "step = 479000: loss = 0.0019275140948593616\n",
      "step = 479000: Average Return = -168.39999389648438\n",
      "step = 479200: loss = 0.003262391546741128\n",
      "step = 479400: loss = 0.004700609017163515\n",
      "step = 479600: loss = 0.003981775604188442\n",
      "step = 479800: loss = 0.007878688164055347\n",
      "step = 480000: loss = 0.31088775396347046\n",
      "step = 480000: Average Return = -200.0\n",
      "step = 480200: loss = 0.004089672584086657\n",
      "step = 480400: loss = 0.0034591504372656345\n",
      "step = 480600: loss = 0.0011104220757260919\n",
      "step = 480800: loss = 0.004726033192127943\n",
      "step = 481000: loss = 0.004567034542560577\n",
      "step = 481000: Average Return = -200.0\n",
      "step = 481200: loss = 0.0018379349494352937\n",
      "step = 481400: loss = 1.0610798597335815\n",
      "step = 481600: loss = 0.0031412814278155565\n",
      "step = 481800: loss = 0.0049921683967113495\n",
      "step = 482000: loss = 0.003145787864923477\n",
      "step = 482000: Average Return = -152.89999389648438\n",
      "step = 482200: loss = 0.010146684013307095\n",
      "step = 482400: loss = 0.010012834332883358\n",
      "step = 482600: loss = 0.9686586260795593\n",
      "step = 482800: loss = 0.004976256750524044\n",
      "step = 483000: loss = 0.004062614869326353\n",
      "step = 483000: Average Return = -188.0\n",
      "step = 483200: loss = 0.660629391670227\n",
      "step = 483400: loss = 0.0021037140395492315\n",
      "step = 483600: loss = 0.004588720388710499\n",
      "step = 483800: loss = 0.0036233910359442234\n",
      "step = 484000: loss = 0.00364477070979774\n",
      "step = 484000: Average Return = -141.6999969482422\n",
      "step = 484200: loss = 0.005662193521857262\n",
      "step = 484400: loss = 0.008084138855338097\n",
      "step = 484600: loss = 1.1390799283981323\n",
      "step = 484800: loss = 1.2263829708099365\n",
      "step = 485000: loss = 0.0026752480771392584\n",
      "step = 485000: Average Return = -193.8000030517578\n",
      "step = 485200: loss = 0.9676379561424255\n",
      "step = 485400: loss = 0.6036691069602966\n",
      "step = 485600: loss = 0.005627742037177086\n",
      "step = 485800: loss = 0.0041093663312494755\n",
      "step = 486000: loss = 0.00535221491008997\n",
      "step = 486000: Average Return = -200.0\n",
      "step = 486200: loss = 0.003733776742592454\n",
      "step = 486400: loss = 0.0037489784881472588\n",
      "step = 486600: loss = 0.003072245977818966\n",
      "step = 486800: loss = 0.0057165781036019325\n",
      "step = 487000: loss = 1.054866909980774\n",
      "step = 487000: Average Return = -172.5\n",
      "step = 487200: loss = 0.002574671059846878\n",
      "step = 487400: loss = 0.004760619252920151\n",
      "step = 487600: loss = 0.7146544456481934\n",
      "step = 487800: loss = 0.003331343410536647\n",
      "step = 488000: loss = 1.0282526016235352\n",
      "step = 488000: Average Return = -139.60000610351562\n",
      "step = 488200: loss = 1.9536614418029785\n",
      "step = 488400: loss = 0.0014468756271526217\n",
      "step = 488600: loss = 0.72533118724823\n",
      "step = 488800: loss = 0.6480007171630859\n",
      "step = 489000: loss = 0.003149645868688822\n",
      "step = 489000: Average Return = -187.6999969482422\n",
      "step = 489200: loss = 1.0352795124053955\n",
      "step = 489400: loss = 0.004137217067182064\n",
      "step = 489600: loss = 0.007043746765702963\n",
      "step = 489800: loss = 0.004299858585000038\n",
      "step = 490000: loss = 1.2218784093856812\n",
      "step = 490000: Average Return = -200.0\n",
      "step = 490200: loss = 0.002608788199722767\n",
      "step = 490400: loss = 0.0027010792400687933\n",
      "step = 490600: loss = 0.006225138437002897\n",
      "step = 490800: loss = 0.00850586500018835\n",
      "step = 491000: loss = 0.005009254906326532\n",
      "step = 491000: Average Return = -200.0\n",
      "step = 491200: loss = 0.004239958710968494\n",
      "step = 491400: loss = 0.0014018146321177483\n",
      "step = 491600: loss = 0.9228898882865906\n",
      "step = 491800: loss = 0.010514849796891212\n",
      "step = 492000: loss = 0.0038660229183733463\n",
      "step = 492000: Average Return = -200.0\n",
      "step = 492200: loss = 0.010661957785487175\n",
      "step = 492400: loss = 0.00370252737775445\n",
      "step = 492600: loss = 0.010751908645033836\n",
      "step = 492800: loss = 0.004460050258785486\n",
      "step = 493000: loss = 0.004746709950268269\n",
      "step = 493000: Average Return = -134.6999969482422\n",
      "step = 493200: loss = 1.0022655725479126\n",
      "step = 493400: loss = 1.2194944620132446\n",
      "step = 493600: loss = 0.0021356409415602684\n",
      "step = 493800: loss = 1.236476182937622\n",
      "step = 494000: loss = 0.0060735722072422504\n",
      "step = 494000: Average Return = -181.1999969482422\n",
      "step = 494200: loss = 2.0205860137939453\n",
      "step = 494400: loss = 0.005918196402490139\n",
      "step = 494600: loss = 0.0027720765210688114\n",
      "step = 494800: loss = 0.008715543895959854\n",
      "step = 495000: loss = 1.2015694379806519\n",
      "step = 495000: Average Return = -200.0\n",
      "step = 495200: loss = 0.010159810073673725\n",
      "step = 495400: loss = 1.115743637084961\n",
      "step = 495600: loss = 0.0033619690220803022\n",
      "step = 495800: loss = 1.2017866373062134\n",
      "step = 496000: loss = 0.0057146139442920685\n",
      "step = 496000: Average Return = -200.0\n",
      "step = 496200: loss = 0.0063470215536653996\n",
      "step = 496400: loss = 0.006580827757716179\n",
      "step = 496600: loss = 0.0027409703470766544\n",
      "step = 496800: loss = 0.005026339087635279\n",
      "step = 497000: loss = 0.0017749344697222114\n",
      "step = 497000: Average Return = -200.0\n",
      "step = 497200: loss = 0.00954925362020731\n",
      "step = 497400: loss = 0.8823206424713135\n",
      "step = 497600: loss = 0.003071774961426854\n",
      "step = 497800: loss = 0.005088367965072393\n",
      "step = 498000: loss = 0.006476087961345911\n",
      "step = 498000: Average Return = -177.6999969482422\n",
      "step = 498200: loss = 0.9840735793113708\n",
      "step = 498400: loss = 0.0036698104813694954\n",
      "step = 498600: loss = 0.004492308013141155\n",
      "step = 498800: loss = 1.4440125226974487\n",
      "step = 499000: loss = 0.0033364410046488047\n",
      "step = 499000: Average Return = -163.5\n",
      "step = 499200: loss = 1.1235098838806152\n",
      "step = 499400: loss = 0.002962992526590824\n",
      "step = 499600: loss = 1.1282434463500977\n",
      "step = 499800: loss = 0.006795818917453289\n",
      "step = 500000: loss = 0.0027500279247760773\n",
      "step = 500000: Average Return = -133.5\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    %%time\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent.train = common.function(agent.train)\n",
    "\n",
    "# Reset the train step.\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "\n",
    "# Reset the environment.\n",
    "time_step = train_py_env.reset()\n",
    "\n",
    "# Create a driver to collect experience.\n",
    "collect_driver = py_driver.PyDriver(\n",
    "    env,\n",
    "    py_tf_eager_policy.PyTFEagerPolicy(\n",
    "      agent.collect_policy, use_tf_function=True),\n",
    "    [rb_observer],\n",
    "    max_steps=collect_steps_per_iteration)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "\n",
    "    # Collect a few steps and save to the replay buffer.\n",
    "    time_step, _ = collect_driver.run(time_step)\n",
    "\n",
    "    # Sample a batch of data from the buffer and update the agent's network.\n",
    "    experience, unused_info = next(iterator)\n",
    "    train_loss = agent.train(experience).loss\n",
    "\n",
    "    step = agent.train_step_counter.numpy()\n",
    "\n",
    "    if step % log_interval == 0:\n",
    "        print('step = {0}: loss = {1}'.format(step, train_loss))\n",
    "\n",
    "    if step % eval_interval == 0:\n",
    "        avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "        print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "        returns.append(avg_return)\n",
    "\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.10 Visualization and Training Time Determination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One iteration of Mountain Car consists of 200 time steps. The environment gives a reward of -1 for each step the agent takes, so the minimum return for one episode is -200 (which corresponds to the average reward of the random policy). The chart below shows the return increasing from -200 when the number of iterations reaches the value of approximately 240000. From this time step onwards, the agent begins to learn. This is likely due to the decrease in epsilon values that occurs as the training evolves. Thus, the actions start to be less random as the training evolves, with the agent taking advantage of its learned knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "id": "i7b5q8RuQo_r",
    "outputId": "aeda74a9-cf92-4257-cd4c-4ec362207098"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-204.175, 250.0)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "iterations = range(0, num_iterations + 1, eval_interval)\n",
    "plt.plot(iterations, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylim(top=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fakmmH2hvmps",
    "outputId": "1e48e838-25fb-4c8e-e749-decb24972478"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training took 254.0 minutes and 5.902 seconds\n"
     ]
    }
   ],
   "source": [
    "total_time = end_time - start_time\n",
    "minutes = total_time // 60\n",
    "seconds = total_time % 60\n",
    "seconds = round(seconds, 3)\n",
    "print(f'The training took {minutes} minutes and {seconds} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Videos of the trained agent were also created and compared it with the one following a random policy. Since the embedded videos disappear as the Notebook is downloaded from Colab, the download of these videos was performed in addition to rendering them within Colab. The corresponding videos can be found within this project folder and are named \"trained-agent.mp4\" and \"random-agent.mp4\".\n",
    "\n",
    "As observed from the these videos, in all five iterations of the Mountain Car game with the trained agent, the car reached its goal. Comparing the trained agent with the random agent (which follows a random policy), the latter was never able to reach its goal during five iterations of the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "CW0zz5DgQ8Xd"
   },
   "outputs": [],
   "source": [
    "def create_policy_eval_video(policy, filename, num_episodes=5, fps=30):\n",
    "    filename = filename + \".mp4\"\n",
    "    with imageio.get_writer(filename, fps=fps) as video:\n",
    "        for _ in range(num_episodes):\n",
    "            time_step = eval_env.reset()\n",
    "            video.append_data(eval_py_env.render())\n",
    "            while not time_step.is_last():\n",
    "                action_step = policy.action(time_step)\n",
    "                time_step = eval_env.step(action_step.action)\n",
    "                video.append_data(eval_py_env.render())\n",
    "\n",
    "    return files.download(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "SatxdVafwttM",
    "outputId": "59dfb154-f250-4b25-d6c8-19aa7948d9b2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (400, 600) to (400, 608) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to None (risking incompatibility). You may also see a FFMPEG warning concerning speedloss due to data not being aligned.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_1f1c95d8-8d23-48f1-9ba6-786897d6e659\", \"trained-agent.mp4\", 122027)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "create_policy_eval_video(agent.policy, \"trained-agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "N2ylPxZhP0v3",
    "outputId": "1b6c4e6b-18e9-4a1b-b8fb-b723f49f120f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (400, 600) to (400, 608) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to None (risking incompatibility). You may also see a FFMPEG warning concerning speedloss due to data not being aligned.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_5a8b9480-d7be-417c-8b56-01a4f4d305b1\", \"random-agent.mp4\", 145139)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "create_policy_eval_video(random_policy, \"random-agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparing the two trained agents using Q-learning and Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was possible to train two agents for the Mountain Car problem using both Q-learning and Deep Q-Learning. In the former case, it was necessary to perform discretization of the observation space to perform training. For the \"best produced\" agent, the training took almost 8 minutes. In all five iterations of the Car Mountain game, this trained agent following our approximated best policy was able to reach its goal, taking between 115 to 158 steps. For the agent trained with Deep Q-Learning, in five iterations of the Car Mountain game the trained agent was also able to reach its goal. The training in this case took considerably longer Q-learning at slightly more than 254 minutes (more than 4 hours).\n",
    "\n",
    "To conclude, it appears that deep reinforcement learning using Q-learning outperforms Deep Q-learning for the Mountain Car problem, since the training time of the former was much lower than the latter with comparable results. Indeed, training neural networks to estimate Q-values for the Mountain Car problem took much longer than simply using tabular Q-learning for this task. This might be due to the fact that the Mountain Car environment is relatively simple with a low-dimensional state space. It should also be noted that it was considerably easier to train the agent with Q-learning in comparision with Deep Q-Learning. In addition to the already mentioned training duration, the `Reverb` replay system only works in Colab or Linux-based operating systems, which made the training with Deep Q-Learning more difficult since Colab would periodically restart during execution time."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
